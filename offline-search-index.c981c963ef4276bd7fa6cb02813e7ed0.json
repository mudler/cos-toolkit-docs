




























































[{"body":"This collection contains GRUB2 configurations and related installation artifacts.\n system/grub2-efi-image ships a generic efi image for the repository platform system/grub2-artifacts ships grub files which typically are installed into the GRUB partition ststem/grub2-config ships default cOS GRUB configuration file  ","categories":"","description":"Packages collection that contains GRUB configuration and artifacts\n","excerpt":"Packages collection that contains GRUB configuration and artifacts\n","ref":"/docs/reference/packages/grub/","tags":"","title":"GRUB"},{"body":"In the cOS repository are present meta packages that are collection of packages which are suitable for different needs.\nWe can compare them to bundles which pulls in during installation sub-packages which should be installed in the system target.\nFor example, we have cos-minimal and cos-core: cos-core is the minimum required of packages in order for having a working cOS derivative, while cos-minimal have additional cloud-init configurations that allows a system in order to boot properly.\n","categories":"","description":"Meta packages are collection of packages, like bundles, which installs a collection of sub-packages\n","excerpt":"Meta packages are collection of packages, like bundles, which installs …","ref":"/docs/reference/packages/meta-packages/","tags":"","title":"Meta packages"},{"body":"cOS-toolkit releases consist on container images that can be used to build derived against and the cos source tree itself.\ncOS supports different release channels, all the final and cache images used are tagged and pushed regularly to Quay Container Registry and can be pulled for inspection from the registry as well.\nThose are exactly the same images used during upgrades, and can also be used to build Linux derivatives from cOS.\nFor example, if you want to see locally what’s in a openSUSE cOS version, you can:\n$ docker run -ti --rm quay.io/costoolkit/releases-green:cos-system-$VERSION /bin/bash Download cOS You can also try out cOS from the vanilla images and use it to experiment locally or either bootstrap a derivative: those are minimal system with a small package set in order to boot and deploy a container.\nLatest cOS-toolkit releases assets (ISOs, Raw disks, Cloud images) can be found on Github, check Booting for an explanation of each asset type and how to use it.\ncOS can run in: VMs, baremetals and Cloud - the default login username/password is root/cos.\nInstall To install run elemental install \u003cdevice\u003e to start the installation process. Remove the ISO/medium and reboot.\nNote: elemental install supports other options as well. Run elemental install --help to see a complete help.\nReleases cOS has 3 variants:\n green: openSUSE based one, shipping packages from OpenSUSE Leap 15.3 repositories. blue: Fedora based one, shipping packages from Fedora 33 repositories orange: Ubuntu based one, shipping packages form Ubuntu 20.10 repositories  We currently support and test only the green variant.\nPublished AMI images We publish AMI images for each release, you can find them into ec2 for example with:\naws ec2 describe-images --filters 'Name=description,Values=cOS*' The list of all the published AMI is released as part of the releases assets with the ami_id.txt.tar.xz file, e.g. v0.6.7\nThe AMI Owner ID is 053594193760.\nWhat to do next? Check out the customization section to customize cOS or the tutorial section for some already prepared recipe examples.\n","categories":"","description":"How to get cOS vanilla assets: ISOs, Cloud Images, Vagrant boxes, ....\n","excerpt":"How to get cOS vanilla assets: ISOs, Cloud Images, Vagrant boxes, .... …","ref":"/docs/getting-started/download/","tags":"","title":"Download"},{"body":"When building a cos-toolkit derivative, a common set of packages are provided already with a common default configuration. Some of the most notably are:\n systemd as init system grub for boot loader dracut for initramfs  Each cos-toolkit flavor (opensuse, ubuntu, fedora) ships their own set of base packages depending on the distribution they are based against. You can find the list of packages in the packages keyword in the corresponding values file for each flavor\n","categories":"","description":"Package stack for derivatives\n","excerpt":"Package stack for derivatives\n","ref":"/docs/creating-derivatives/package_stack/","tags":"","title":"Package stack"},{"body":"We have a custom augmented cloud-init syntax that allows to hook into various stages of the system, for example:\n Initramfs load Boot Network availability During upgrades, installation, deployments , and resets  Cloud-init files in /system/oem, /oem and /usr/local/oem are applied in 5 different phases: boot, network, fs, initramfs and reconcile. All the available cloud-init keywords can be used in each stage. Additionally, it’s possible also to hook before or after a stage has run, each one has a specific stage which is possible to run steps: boot.after, network.before, fs.after etc.\nMultiple stages can be specified in a single cloud-init file.\nNote When a cOS derivative boots it creates sentinel files in order to allow to execute cloud-init steps programmaticaly.\n /run/cos/recovery_mode is being created when booting from the recovery partition /run/cos/live_mode is created when booting from the LiveCD  To execute a block using the sentinel files you can specify: if: '[ -f \"/run/cos/...\" ]', see the examples below.\n Stages Below there is a detailed list of the stages available that can be used in the cloud-init configuration files\nrootfs This is the earliest stage, running before switching root, just right after the root is mounted in /sysroot and before applying the immutable rootfs configuration. This stage is executed over initrd root, no chroot is applied.\nExample:\nname:\"Set persistent devices\"stage:rootfs:- name:\"Layout configuration\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"initramfs This is still an early stage, running before switching root. Here you can apply radical changes to the booting setup of cOS. Despite this is executed before switching root this exection runs chrooted into the target root after the immutable rootfs is set up and ready.\nExample:\nname:\"Run something on initramfs\"stages:initramfs:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive touch /etc/something_important- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeboot This stage is executed after initramfs has switched root, during the systemd bootup process.\nExample:\nname:\"Run something on boot\"stages:boot:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modefs This stage is executed when fs is mounted and is guaranteed to have access to COS_STATE and COS_PERSISTENT.\nExample:\nname:\"Run something on boot\"stages:fs:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |touch /usr/local/something- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modenetwork This stage is executed when network is available\nExample:\nname:\"Run something on boot\"stages:network:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Network is available, do something..reconcile This stage is executed 5m after boot and periodically each 60m.\nExample:\nname:\"Run something on boot\"stages:reconcile:- name:\"Setting\"if:'[ ! -f \"/run/sentinel\" ]'commands:- |touch /run/sentinelafter-install This stage is executed after installation of the OS has ended (last step of elemental install).\nExample:\nname:\"Run something after installation\"stages:after-install:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeafter-install-chroot This stage is executed after installation of the OS has ended (last step of elemental install). Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software.  Example:\nname:\"Run something after installation\"stages:after-install-chroot:- name:\"Setting\"commands:- |...after-upgrade This stage is executed after upgrade of the OS has ended (last step of elemental upgrade).\nExample:\nname:\"Run something after upgrade\"stages:after-upgrade:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeafter-upgrade-chroot This stage is executed after upgrade of the OS has ended (after calling elemental upgrade). Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software.  Example:\nname:\"Run something after installation\"stages:after-upgrade-chroot:- name:\"Setting\"commands:- |...after-reset This stage is executed after reset of the OS has ended (last step of elemental reset).\nExample:\nname:\"Run something after reset\"stages:after-reset:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeafter-reset-chroot This stage is executed after reset of the OS has ended (last step of elemental reset). Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software.  Example:\nname:\"Run something after installation\"stages:after-reset-chroot:- name:\"Setting\"commands:- |...after-deploy This stage is executed after deployment of the OS has ended (after calling cos-deploy).\nExample:\nname:\"Run something after deployment\"stages:after-deploy:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modeafter-deploy-chroot This stage is executed after deployment of the OS has ended (after calling cos-deploy). Note Steps executed at this stage are running inside the new OS as chroot, allowing to write persisting changes to the image, for example by downloading and installing additional software.  Example:\nname:\"Run something after installation\"stages:after-deploy-chroot:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modebefore-install This stage is executed before installation (executed during elemental install).\nExample:\nname:\"Run something before installation\"stages:before-install:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modebefore-upgrade This stage is executed before upgrade of the OS (executed during elemental upgrade).\nExample:\nname:\"Run something before upgrade\"stages:before-upgrade:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modebefore-reset This stage is executed before reset of the OS (executed during elemental reset).\nExample:\nname:\"Run something before reset\"stages:before-reset:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery modebefore-deploy This stage is executed before deployment of the OS has ended (executed during cos-deploy).\nExample:\nname:\"Run something before deployment\"stages:before-deploy:- name:\"Setting\"if:'[ ! -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in active or passive- name:\"Setting\"if:'[ -f \"/run/cos/recovery_mode\" ]'commands:- |# Run something when we are booting in recovery mode","categories":"","description":"Configure the system in the various stages: boot, initramfs, fs, network, reconcile\n","excerpt":"Configure the system in the various stages: boot, initramfs, fs, …","ref":"/docs/customizing/stages/","tags":"","title":"Stages"},{"body":"cOS provides a runtime and buildtime framework in order to boot containers in VMs, Baremetals and Cloud.\nYou can either choose to build a cOS derivative or run cOS to boostrap a new system.\ncOS vanilla images are published to allow to deploy user-built derivatives.\ncOS is designed to run, deploy and upgrade derivatives that can be built just as standard OCI container images. cOS assets can be used to either drive unattended deployments of a derivative or used to create custom images (with packer).\nPhilosophy Philosophy behind cos-toolkit is simple: it allows you to create Linux derivatives from container images.\n Container registry as a single source of truth Hybrid way to access your image for different scopes (development, debugging, ..) No more inconsistent states between nodes. A “Store” to keep your (tagged) shared states where you can rollback and upgrade into. “Stateless”: Images with upgrades are rebuilt from scratch instead of applying upgrades.  A derivative which includes cos-toolkit, in runtime can:\n Upgrade to another container image Deploy a system from scratch from an image Reset or recovery to an Image Customize the image during runtime to persist changes across reboots Perform an installation from Live medium  The container image, seamlessly:\n is booted as-is, encapsulating all the needed components (kernel, initrd, cos-toolkit, ecc) can be pulled locally for inspection, development and debugging can be used to create installation medium as ISO, Raw images, OVA, Cloud  Build cOS derivatives The starting point to use cos-toolkit is to check out our examples and our creating bootable images section.\nThe only requirement to build derivatives with cos-toolkit is Docker installed. If you are interested in building cOS-toolkit itself, see Development notes.\nThe toolkit itself is delivered as a set of standalone, re-usable OCI artifacts which are tagged and tracked as standard OCI images and it is installed inside the container image to provide the same featureset among derivatives, see how to create bootable images.\nWhat to do next? Check out how to create bootable images or download the cOS vanilla images to give cOS a try!\n","categories":"","description":"Getting started with cOS\n","excerpt":"Getting started with cOS\n","ref":"/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"cOS (or any cOS derivative built with cos-toolkit) can be installed with elemental install:\nelemental install [options] \u003cdevice\u003e    Option Description     –cloud-init string Cloud-init config file   –cosign Enable cosign verification (requires images with signatures)   –cosign-key string Sets the URL of the public key to be used by cosign validation   –directory string Use directory as source to install from   –docker-image string Install a specified container image   –force Force install   –force-efi Forces an EFI installation   –force-gpt Forces a GPT partition table   –help help for install   –iso string Performs an installation from the ISO url   –no-format Don’t format disks. It is implied that COS_STATE, COS_RECOVERY, COS_PERSISTENT, COS_OEM are already existing   –no-verify Disable mtree checksum verification (requires images manifests generated with mtree separately)   –partition-layout string Partitioning layout file   –poweroff Shutdown the system after install   –reboot Reboot the system after install   –strict Enable strict check of hooks (They need to exit with 0)   –tty Add named tty to grub    Custom OEM configuration During installation it can be specified a cloud-init config file, that will be installed and persist in the system after installation:\nelemental install --cloud-init [url|path] \u003cdevice\u003e Custom partitioning layout When installing with GPT or EFI it’s possible to specify a custom partitioning layout via specific config file, e.g.:\nstages:partitioning:- name:\"Repart disk\"layout:device:path:/dev/sdaadd_partitions:- fsLabel:COS_STATEsize:8192pLabel:state- fsLabel:COS_OEMsize:10pLabel:oem- fsLabel:COS_RECOVERY# default filesystem is ext2 if omittedfilesystem:ext4size:40000pLabel:recovery- fsLabel:COS_PERSISTENTpLabel:persistent# default filesystem is ext2 if omittedfilesystem:ext4size:40000Refer to the cloud-init config file reference about the layout section.\nIt can be also used to create additional partitions, or either create partitions into a different device, etc..\nRun the installer with\nelemental install --partition-layout \u003cfile\u003e \u003cdevice\u003e  Note While specifying a custom layout it is necessary to at least create 4 partitions: COS_OEM, COS_STATE, COS_RECOVERY, COS_PERSISTENT. Keep in mind the following while adjusting the partition sizes manually:\n COS_OEM typically is used to store cloud-init files, so it can be also small. COS_STATE is used to store all the system images, which by default are set to 3GB. This value is customizable in our configuration file. A system may contain 2 images (Active/Passive), plus additional space for a third transitioning image which will be created during upgrades. COS_RECOVERY contains the recovery image, and additional space for a transition image during upgrades COS_PERSISTENT is the persistent partition that is mounted over /usr/local. Typically is set to take all the free space left.   Installation from 3rd party LiveCD or rescue mediums The installer can be used to perform installations also from outside the cOS or standard derivative ISOs.\nFor instance, it is possible to install cOS (or any derivative) with the installer from another bootable medium, or a rescue mode which is booting from RAM, given there is enough free RAM available.\nWith Docker If in the rescue system, or LiveCD you have docker available, it can be used to perform an installation\ndocker run --privileged -v $DEVICE:$DEVICE -ti quay.io/costoolkit/releases-green:cos-system-0.7.4-2 elemental install --docker-image $IMAGE $DEVICE Where $IMAGE is the container image that we want to install (e.g. quay.io/costoolkit/releases-green:cos-system-0.7.4-2 ), and $DEVICE is the the device where to perform the installation to (e.g. /dev/sda).\nNote, we used the quay.io/costoolkit/releases-green:cos-system-0.7.4-2 image which contains the installer and the dependencies, similarly another derivative or another cos-system version can be used.\nBy using manually the Elemental installer Similarly, the same mechanism can be used without docker. Download elemental from github releases and run the follow as root:\nelemental install --docker-image $IMAGE $DEVICE ","categories":"","description":"Installing cOS or a derivative locally\n","excerpt":"Installing cOS or a derivative locally\n","ref":"/docs/getting-started/install/","tags":"","title":"Installing"},{"body":"Cosign is a project that signs and verifies containers and stores the signatures on OCI registries.\nYou can check the cosign github repo for more information.\nIn cos-toolkit we sign every container that we generate as part of our publish process so the signature can be verified during package installation with luet or during deploy/upgrades from a deployed system to verify that the containers have not been altered in any way since their build.\nCurrently cosign provides 2 methods for signing and verifying.\n private/public key keyless  We use keyless signatures based on OIDC Identity tokens provided by github, so nobody has access to any private keys and can use them. (For more info about keyless signing/verification check here)\nThis signature generation is provided by luet-cosign which is a luet plugin that generates the signatures on image push when building, and verifies them on package unpack when installing/upgrading/deploying.\nThe process is completely transparent to the end user when upgrading/deploying a running system and using our published artifacts.\nWhen using luet-cosign as part of luet install you need to set COSIGN_REPOSITORY=raccos/releases-green and COSIGN_EXPERIMENTAL=1 so it can find the proper signatures and use keyless verification\nNote Currently setting COSIGN_REPOSITORY value is due to quay.io not supporting OCI artifacts. It may be removed in the future and signatures stored along the artifacts.  Derivatives If building a derivative, you can also sign and verify you final artifacts with the use of luet-cosign.\nAs keyless is only possible to do in an CI environment (as it needs an OIDC token) you would need to set up private/public signature and verification.\nNote If you are building and publishing your derivatives with luet on github, you can see an example on how we generate and push the keyless signatures ourselves on this workflow  Verify cos-toolkit artifacts as part of derivative building If you consume cos-toolkit artifacts in your Dockerfile as part of building a derivative you can verify the signatures of the artifacts by setting:\nENV COSIGN_REPOSITORY=raccos/releases-greenENV COSIGN_EXPERIMENTAL=1 RUN luet install -y meta/cos-verify # install dependencies for signature checking Note The\n meta/cos-verify  is a meta package that will pull\n toolchain/cosign  and\n toolchain/luet-cosign  .\n And then making sure you call luet with --plugin luet-cosign. You can see an example of this in our standard Dockerfile example\nThat would verify the artifacts coming from our repository.\nFor signing resulting containers with a private/public key, please refer to the cosign documents.\nFor verifying with a private/public key, the only thing you need is to set the env var COSIGN_PUBLIC_KEY_LOCATION to point to the public key that signed and enable the luet-cosign plugin.\nNote Currently there is an issue in which if there is more than one repo and one of those repos is not signed the whole install will fail due to cosign failing to verify the unsigned repo.\nIf you are using luet with one or more unsigned repos, it’s not possible to use cosign to verify the chain.\nPlease follow up in https://github.com/rancher-sandbox/luet-cosign/issues/6 for more info.\n ","categories":"","description":"How we use cosign in cos-toolkit\n","excerpt":"How we use cosign in cos-toolkit\n","ref":"/docs/creating-derivatives/cosign/","tags":"","title":"Cosign"},{"body":" Each cOS release contains a variety of assets:\n ISOs cOS-Seed-green-$VERSION-$ARCH.iso.tar.xz QCOW cOS-Packer-green_$VERSION-QEMU-$ARCH.tar.gz.tar.xz OVA cOS-Packer-green_$VERSION-vbox-$ARCH.tar.gz.tar.xz Vagrant box (vbox provider) cOS-Packer-green-$VERSION-vbox-$ARCH.box.tar.xz Vagrant box (qemu provider) cOS-Packer-green-$VERSION-QEMU-$ARCH.box.tar.xz RAW Disk cOS-Vanilla-RAW-$VERSION-$ARCH.raw.tar.xz VHD cOS-Vanilla-AZURE-$VERSION-$ARCH.vhd.tar.xz GCE cOS-Vanilla-GCE-green-$VERSION-$ARCH.raw.tar.xz  here we try to summarize and document how they are meant to be consumed.\nISO ISO images (e.g. ``cOS-Seed-green-$VERSION-$ARCH.iso.tar.xz` ) are shipping a cOS vanilla image and they feature an installer to perform an automated installation. They can be used to burn USB sticks or CD/DVD used to boot baremetals. Once booted, you can install cOS with:\nelemental install $DEVICE See also [../install] for installation options.\nAfter the first boot you can also switch to a derivative by:\nelemental upgrade --docker-image --no-verify $IMAGE Unattended deployment After booting the iso, it is possible to deploy directly an image of choice with cos-deploy\ncos-deploy --docker-image $IMAGE Booting from network You can boot a cOS squashfs by using a native iPXE implementation on your system or by inserting a custom build iPXE iso by using your either your servers management web-interface (ILO, iDrac, …) or using a redfish library or its vendor specific implementation (e.g. ilorest).\nTo boot from IPXE you need to extract the squashfs from images with the cos-img-recovery- prefix. By pulling the image, saving it using docker save, then unpack the single layer in the file to an extra directory and then run\n$\u003e mksquashfs \u003cpath_to_folder\u003e \u003coutput_path\u003e/root.squashfs Note: The squashfs file can also be extracted using the following docker command:\n$\u003e docker run -v $PWD:/cOS --entrypoint /usr/bin/luet -ti --rm quay.io/costoolkit/toolchain util unpack quay.io/costoolkit/releases-green:cos-img-recovery-\u003cversion\u003e . Then copy the boot directory and the squashfs file to your webserver and use the following script to boot.\n#!ipxe  ifconf kernel http://\u003cweb_server_ip\u003e/boot/vmlinuz ip=dhcp rd.cos.disable rd.noverifyssl root=live:http://\u003cweb_server_ip\u003e/root.squashfs console=ttyS0 console=tty1 cos.setup=http://\u003cweb_server_ip\u003e/\u003cpath_to_cloud_config.yaml\u003e initrd http://\u003cweb_server_ip\u003e/boot/initrd boot To build a custom iPXE image clone the source from https://github.com/ipxe/ipxe, then traverse into the src directory and run make EMBED=\u003c/path/to/your/script.ipxe\u003e. The resulting iso can be found in src/bin/ipxe.iso and should be ~1MB in size.\nVirtual machines For booting into Virtual machines we offer QCOW2, OVA, and raw disk recovery images which can be used to bootstrap your booting container.\nQCOW2 QCOW2 images ( e.g. cOS-Packer-green-$VERSION-QEMU-$ARCH.tar.gz.tar.xz ) contains a pre-installed cOS system which can be booted via QEMU, e.g:\nqemu-system-x86_64 -m 2048 -hda cOS -nographic OVA Ova images ( e.g. cOS-Packer-green-$VERSION-vbox-$ARCH.tar.gz.tar.xz ) contains a pre-installed cOS system which can be booted via vbox. Please check the virtuabox docs on how to create a new VM with an existing disk.\nVagrant Download the vagrant box artifact ( e.g. cOS-Packer-green-$VERSION-{vbox, QEMU}-$ARCH.box.tar.xz ), extract it and run:\nvagrant box add cos \u003ccos-box-image\u003e vagrant init cos vagrant up RAW disk images RAW disk images ( e.g. cOS-Vanilla-RAW-green-$VERSION-$ARCH.raw.tar.xz ) contains only the cOS recovery system. Those are typically used when creating derivatives images based on top of cOS.\nThey can be run with QEMU with:\nqemu-system-x86_64 -m 2048 -hda \u003ccos-disk-image\u003e.raw -bios /usr/share/qemu/ovmf-x86_64.bin Cloud Images Cloud images are vanilla images that boots into recovery mode and can be used to deploy whatever image you want to the VM. Then you can snapshot that VM into a VM image ready to deploy with the default cOS system or your derivative.\nAt the moment we support Azure and AWS images among our artifacts. We publish AWS images that can also be re-used in packer templates for creating customized AMI images.\nThe RAW image can then be used into packer templates to generate custom Images, or used as-is with a userdata to deploy a container image of choice with an input user-data.\nImport an AWS image manually You can also use RAW images ( e.g. cOS-Vanilla-RAW-green-$VERSION-$ARCH.raw.tar.xz ) manually when importing AMIs images and use them to generate images with Packer. See build AMI with Packer\n  Upload the raw image to an S3 bucket  aws s3 cp \u003ccos-raw-image\u003e s3://\u003cyour_s3_bucket\u003e Created the disk container JSON (container.json file) as:  { \"Description\": \"cOS Testing image in RAW format\", \"Format\": \"raw\", \"UserBucket\": { \"S3Bucket\": \"\u003cyour_s3_bucket\u003e\", \"S3Key\": \"\u003ccos-raw-image\u003e\" } } Import the disk as snapshot  aws ec2 import-snapshot --description \"cOS PoC\" --disk-container file://container.json  Followed the procedure described in AWS docs to register an AMI from snapshot. Used all default settings unless for the firmware, set to force to UEFI boot.\n  Launch instance with this simple userdata with at least a 16Gb boot disk:\n  name: \"Default deployment\" stages: rootfs.after: - name: \"Repart image\" layout: # It will partition a device including the given filesystem label or part label (filesystem label matches first) device: label: COS_RECOVERY add_partitions: - fsLabel: COS_STATE # 10Gb for COS_STATE, so the disk should have at least 16Gb size: 10240 pLabel: state - fsLabel: COS_PERSISTENT # unset size or 0 size means all available space pLabel: persistent initramfs: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Set sshd to wait for deployment\" files: - path: \"/etc/systemd/system/sshd.service.d/override.conf\" content: | [Unit] After=cos-setup-network.service network: - if: '[ -f \"/run/cos/recovery_mode\" ]' name: \"Deploy cos-system\" commands: - | # Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy \u0026\u0026 shutdown -r now Importing a Google Cloud image manually You need to use the GCE images ( e.g. cOS-Vanilla-GCE-green-$VERSION-$ARCH.raw.tar.xz ) manually when importing GCE images and use them to generate images with Packer. See build GCE with Packer\n  Upload the Google Cloud compressed disk to your bucket  gsutil cp \u003ccos-gce-image\u003e gs://\u003cyour_bucket\u003e/ Import the disk as an image  gcloud compute images create \u003cnew_image_name\u003e --source-uri=\u003cyour_bucket\u003e/\u003ccos-gce-image\u003e --guest-os-features=UEFI_COMPATIBLE Launch instance with this simple userdata with at least a 16Gb boot disk:   See here on how to add user-data to an instance\nname:\"Default deployment\"stages:rootfs.after:- name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERYadd_partitions:- fsLabel:COS_STATE# 10Gb for COS_STATE, so the disk should have at least 16Gbsize:10240pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available spacepLabel:persistentinitramfs:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Set sshd to wait for deployment\"files:- path:\"/etc/systemd/system/sshd.service.d/override.conf\"content:|[Unit] After=cos-setup-network.servicenetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:- |# Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy \u0026\u0026 shutdown -r nowImporting an Azure image manually  You need to use the AZURE images ( e.g. cOS-Vanilla-AZURE-green-$VERSION-$ARCH.raw.tar.xz ) manually when importing Azure images.\n Upload the Azure Cloud VHD disk in .vhda format ( extract e.g. cOS-Vanilla-AZURE-green-0.6.0-g7d04f1db-x86_64.vhd.tar.xz ) to your bucket  az storage copy --source \u003ccos-azure-image\u003e --destination https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003cdestination-cos-azure-image\u003e Import the disk as an image  az image create --resource-group \u003cresource-group\u003e --source https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003ccos-azure-image\u003e --os-type linux --hyper-v-generation v2 --name \u003cimage-name\u003e Launch instance with this simple userdata with at least a 16Gb boot disk:  Hint: There is currently no way of altering the boot disk of an Azure VM via GUI, use the azure-cli to launch the VM with an expanded OS disk\nname:\"Default deployment\"stages:rootfs.after:- name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERYadd_partitions:- fsLabel:COS_STATE# 10Gb for COS_STATE, so the disk should have at least 16Gbsize:10240pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available spacepLabel:persistentinitramfs:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Set sshd to wait for deployment\"files:- path:\"/etc/systemd/system/sshd.service.d/override.conf\"content:|[Unit] After=cos-setup-network.servicenetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:- |# Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy \u0026\u0026 shutdown -r nowLogin By default you can login with the user root and password cos.\nSee the customization section for examples on how to persist username and password changes after installation.\n","categories":"","description":"Documents various methods for booting cOS vanilla images\n","excerpt":"Documents various methods for booting cOS vanilla images\n","ref":"/docs/getting-started/booting/","tags":"","title":"Booting"},{"body":"By default cOS and derivatives are reading and executing cloud-init files in (lexicopgrahic) sequence inside:\n /system/oem /usr/local/cloud-config /oem  It is also possible to run cloud-init file in a different location (URLs included, too) from boot cmdline by using the cos.setup=.. option.\nNote It is possible to install a custom cloud-init style file during install with --cloud-init flag on elemental install command or, it’s possible to add one or more files manually inside the /oem directory after installation.  While /system/oem is reserved for system configurations to be included directly in the derivative container image, the /oem folder instead is reserved for persistent cloud-init files that can be extended in runtime.\nFor example, if you want to change /etc/issue of the system persistently, you can create /usr/local/cloud-config/90_after_install.yaml or alternatively in /oem/90_after_install.yaml with the following content:\n# The following is executed before fs is setted up:stages:fs:- name:\"After install\"files:- path:/etc/issuecontent:|Welcome, have fun!permissions:0644owner:0group:0systemctl:disable:- wicked- name:\"After install (second step)\"files:- path:/etc/motdcontent:|Welcome, have more fun!permissions:0644owner:0group:0For more examples you can find /system/oem inside cOS vanilla images containing files used to configure on boot a pristine cOS.\n","categories":"","description":"Persisting configurations in cOS and derivatives\n","excerpt":"Persisting configurations in cOS and derivatives\n","ref":"/docs/customizing/configuration_persistency/","tags":"","title":"Configuration persistency"},{"body":"A derivative is a simple container image which can be processed by the cOS toolkit in order to be bootable and installable. This section describes the requirements to create a container image that can be run by cOS.\nRequirements Bootable images are standard container images, that means the usual build and push workflow applies, and building images is also a way to persist oem customizations.\nThe base image can be any Linux distribution that is compatible with our flavors.\nThe image needs to ship:\n parts of the cos-toolkit (required, see below) kernel (required) initrd (required) grub (required) dracut (optional, kernel and initrd can be consumed from the cOS repositories) microcode (optional, not required in order to boot, but recomended) cosign and luet-cosign packages (optional, required if you want to verify the images installed by luet)  Example An illustrative example can be:\nARG LUET_VERSION=0.16.7FROMquay.io/luet/base:$LUET_VERSION AS luetFROMopensuse/leap:15.3 # or Fedora, UbuntuARG ARCH=amd64 ENV ARCH=${ARCH}ENV COSIGN_EXPERIMENTAL=1 # keyless verifyENV COSIGN_REPOSITORY=raccos/releases-green # repo with the signaturesRUN zypper in -y ... # apt-get, dnf...# Here we install cosign and luet-cosign so we can verify that the images installed have been signedRUN luet install -y meta/cos-verify# That's where we install the minimal cos-toolkit meta-package (which pulls the minimal packages needed in order to boot)# note that we are setting `--plugin luet-cosign` so luet verifies the correct signatures of the packages on installRUN luet install --plugin luet-cosign -y meta/cos-minimal# Other custom logic. E.g, customize statically the upgrade channel, default users, packages....In the example above, the cos-toolkit parts that are required are pulled in by RUN luet install -y meta/cos-minimal.\n  meta/cos-minimal  is a meta-package that will pull  toolchain/luet  ,  toolchain/yip  ,  utils/installer  ,  system/cos-setup  ,  system/immutable-rootfs  ,  system/base-dracut-modules  ,  system/grub2-config  ,  system/cloud-config  .\nNote  system/cloud-config  is optional, but provides cOS defaults setting, like default user/password and so on. If you are not installing it directly, an equivalent cloud-config has to be provided in order to properly boot and run a system, see oem configuration.  Using cosign in your derivative The  meta/cos-verify  is a meta package that will pull  toolchain/cosign  and  toolchain/luet-cosign  .\n  toolchain/cosign  and  toolchain/luet-cosign  are optional packages that would install cosign and luet-cosign in order to verify the packages installed by luet.\nYou can use cosign to both verify that packages coming from cos-toolkit are verified and sign your own derivative artifacts\nNote If you want to manually verify cosign and luet-cosign packages before installing them with luet, you can do so by:\n Install Cosign Export the proper vars  export COSIGN_EXPERIMENTAL=1 for keyless verify export COSIGN_REPOSITORY=raccos/releases-green to point cosign to the repo the signatures are stored on   Manually verify the signatures on both packages  Check the latest $VERSION for both packages at the repo (i.e. https://quay.io/repository/costoolkit/releases-green?tab=tags) cosign verify quay.io/costoolkit/releases-green:luet-cosign-toolchain-$VERSION cosign verify quay.io/costoolkit/releases-green:cosign-toolchain-$VERSION     For more info, check the cosign page.\nInitrd The image should provide at least grub, systemd, dracut, a kernel and an initrd. Those are the common set of packages between derivatives. See also package stack. By default the initrd is expected to be symlinked to /boot/initrd and the kernel to /boot/vmlinuz, otherwise you can specify a custom path while building an iso and by customizing grub.\n  system/base-dracut-modules  is required to be installed with luet in case you are building manually the initrd from the Dockerfile and also to run dracut to build the initrd, the command might vary depending on the base distro which was chosen.\n  system/kernel  and  system/dracut-initrd  can also be installed if you plan to use kernels and initrd from the cOS repositories and don’t build them / or install them from the official distro repositories (e.g. with zypper, or dnf or either apt-get…). In this case you don’t need to generate initrd on your own, neither install the kernel coming from the base image.\nBuilding The workflow would be then:\n docker build the image docker push the image to some registry elemental upgrade --no-verify --docker-image $IMAGE from a cOS machine or (cos-deploy if bootstrapping a cloud image)  The following can be incorporated in any standard gitops workflow.\nYou can explore more examples in the example section on how to create bootable images.\nWhat’s next? Now that we have created our derivative container, we can either:\n Build an iso Build an Amazon Image Build a Google Cloud Image  ","categories":"","description":"This document describes the requirements to create standard container images that can be used for `cOS` deployments\n","excerpt":"This document describes the requirements to create standard container …","ref":"/docs/creating-derivatives/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"cOS and derivatives are immutable systems. That means that any change in the running OS will not persist after a reboot.\nWhile configurations can be persisted, there are occasions where installing a custom package or provide additional persistent files in the end system is needed.\nWe will see here a way to install packages, drivers, or apply any modification we might want to do in the OS image during runtime, without any need to rebuild the derivative container image. This will let any user (and not derivative developer) to apply any needed customization and to be able to persist across upgrades.\nTransient changes To apply transient changes, it’s possible to boot a cOS derivative in read/write mode by specifying rd.cos.debugrw see here for more details. This allows to do any change and will persist into the active/passive booting system (does NOT apply for recovery). Altough this methodology should be only considered for debugging purposes.\nPersist changes with Cloud init files Note: The following applies only to derivatives with  utils/installer  at version 0.17 or newer\ncOS allows to apply a set of commands, or cloud-init steps, during upgrade, deploy, install and reset in the context of the target image, in RW capabilities. This allows to carry on changes during upgrades on the target image without the need to re-build or have a custom derivative image.\nAll the configuration that we want to apply to the system will run each time we do an upgrade, a reset or an installation on top of the new downloaded image (in case of upgrade) or the image which is the target system.\nBetween the available stages in the cloud-init there are after-upgrade-chroot, after-install-chroot, after-reset-chroot and after-deploy-chroot, for example, consider the following cloud-init file:\nstages:name:\"Install something\"stages:after-upgrade-chroot:- commands:- zypper in -y ...after-reset-chroot:- commands:- zypper in -y ...after-deploy-chroot:- commands:- zypper in -y ...after-install-chroot:- commands:- zypper in -y ...It will run the zypper in -y ... calls during each stage, in the context of the target system, allowing to customize the target image with additional packages.\nNote zypper calls here are just an example. We could have used dnf for fedora based, or apt-get for ubuntu based images.  When running the cloud-init steps the /oem partition and /usr/local will be mounted to COS_OEM and COS_PERSISTENT respectively, allowing to load extra data (e.g. rpm files, or configuration).\nExample If an user wants to install an additional package in the running system, and keep having that persistent across upgrades, he can copy the following file (install.yaml) inside the /oem folder, or /usr/local/cloud-config:\nstages:name:\"Install something\"stages:after-upgrade-chroot:- commands:- zypper in -y vimand run elemental upgrade.\nIt will automatically upgrade the system with the changes above included.\n","categories":"","description":"Applying changes to cOS images in runtime or “how to install a package in an immutable OS at runtime?”\n","excerpt":"Applying changes to cOS images in runtime or “how to install a package …","ref":"/docs/customizing/runtime_persistent_changes/","tags":"","title":"Runtime persistent changes"},{"body":"Below is a reference of all keys available in the cloud-init style files.\nstages:# \"network\" is the stage where network is expected to be up# It is called internally when network is available from # the cos-setup-network unit.network:# Here there are a list of # steps to be run in the network stage- name:\"Some setup happening\"files:- path:/tmp/foocontent:|testpermissions:0777owner:1000group:100commands:- echo \"test\"modules:- nvidiaenvironment:FOO:\"bar\"systctl:debug.exception-trace:\"0\"hostname:\"foo\"systemctl:enable:- foodisable:- barstart:- bazmask:- foobarauthorized_keys:user:- \"github:mudler\"- \"ssh-rsa ....\"dns:path:/etc/resolv.confnameservers:- 8.8.8.8ensure_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"delete_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"pass\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"datasource:providers:- \"aws\"- \"digitalocean\"path:\"/etc/cloud-data\"The default cloud-config format is split into stages (initramfs, boot, network, initramfs, reconcile, called generically STAGE_ID below) see also stages that are emitted internally during the various phases by calling cos-setup STAGE_ID. steps (STEP_NAME below) defined for each stage are executed in order.\nEach cloud-config file is loaded and executed only at the apprioriate stage, this allows further components to emit their own stages at the desired time.\nThe cloud-init tool can be also run standalone, this helps debugging locally and also during development, you can find separate releases here, or just run it with docker:\ncat \u003c\u003cEOF | docker run -i --rm quay.io/costoolkit/releases-green:cos-recovery-0.6.0 yip -s test - stages: test: - commands: - echo \"test\" EOF   Note: Each cloud-init option can be either run in dot notation ( e.g. stages.network[0].authorized_keys.user=github:user ) in the boot args or either can supply a cloud-init URL at boot with the cos.setup=$URL parameter.\nCompatibility with Cloud Init format A subset of the official cloud-config spec is implemented.\nIf a yaml file starts with #cloud-config it is parsed as a standard cloud-init and automatically associated it to the boot stage. For example:\n#cloud-configgrowpart:mode:autodevices:['/']users:- name:\"bar\"passwd:\"foo\"lock_passwd:trueuid:\"1002\"groups:\"users\"ssh_authorized_keys:- faaapploossh_authorized_keys:- asddruncmd:- foohostname:\"bar\"write_files:- encoding:b64content:CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4path:/foo/barpermissions:\"0644\"owner:\"bar\"Is executed at boot, by using the standard cloud-config format.\nstages.STAGE_ID.STEP_NAME.name A description of the stage step. Used only when printing output to console.\nstages.STAGE_ID.STEP_NAME.files A list of files to write to disk.\nstages:default:- files:- path:/tmp/barcontent:|#!/bin/sh echo \"test\"permissions:0777owner:1000group:100stages.STAGE_ID.STEP_NAME.directories A list of directories to be created on disk. Runs before files.\nstages:default:- name:\"Setup folders\"directories:- path:\"/etc/foo\"permissions:0600owner:0group:0stages.STAGE_ID.STEP_NAME.dns A way to configure the /etc/resolv.conf file.\nstages:default:- name:\"Setup dns\"dns:nameservers:- 8.8.8.8- 1.1.1.1search:- foo.baroptions:- ..path:\"/etc/resolv.conf.bak\"stages.STAGE_ID.STEP_NAME.hostname A string representing the machine hostname. It sets it in the running system, updates /etc/hostname and adds the new hostname to /etc/hosts.\nstages:default:- name:\"Setup hostname\"hostname:\"foo\"stages.STAGE_ID.STEP_NAME.sysctl Kernel configuration. It sets /proc/sys/\u003ckey\u003e accordingly, similarly to sysctl.\nstages:default:- name:\"Setup exception trace\"systctl:debug.exception-trace:\"0\"stages.STAGE_ID.STEP_NAME.authorized_keys A list of SSH authorized keys that should be added for each user. SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME}, similarly for Gitlab with gitlab:${USERNAME}.\nstages:default:- name:\"Setup exception trace\"authorized_keys:mudler:- github:mudler- ssh-rsa:...stages.STAGE_ID.STEP_NAME.node If defined, the node hostname where this stage has to run, otherwise it skips the execution. The node can be also a regexp in the Golang format.\nstages:default:- name:\"Setup logging\"node:\"bastion\"stages.STAGE_ID.STEP_NAME.users A map of users and user info to set. Passwords can be also encrypted.\nThe users parameter adds or modifies the specified list of users. Each user is an object which consists of the following fields. Each field is optional and of type string unless otherwise noted. In case the user is already existing, the entry is ignored.\n name: Required. Login name of user gecos: GECOS comment of user passwd: Hash of the password to use for this user. Unencrypted strings are supported too. homedir: User’s home directory. Defaults to /home/name no-create-home: Boolean. Skip home directory creation. primary-group: Default group for the user. Defaults to a new group created named after the user. groups: Add user to these additional groups no-user-group: Boolean. Skip default group creation. ssh-authorized-keys: List of public SSH keys to authorize for this user system: Create the user as a system user. No home directory will be created. no-log-init: Boolean. Skip initialization of lastlog and faillog databases. shell: User’s login shell.  stages:default:- name:\"Setup users\"users:bastion:passwd:\"strongpassword\"homedir:\"/home/foostages.STAGE_ID.STEP_NAME.ensure_entities A user or a group in the entity format to be configured in the system\nstages:default:- name:\"Setup users\"ensure_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"stages.STAGE_ID.STEP_NAME.delete_entities A user or a group in the entity format to be pruned from the system\nstages:default:- name:\"Setup users\"delete_entities:- path:/etc/passwdentity:|kind: \"user\" username: \"foo\" password: \"x\" uid: 0 gid: 0 info: \"Foo!\" homedir: \"/home/foo\" shell: \"/bin/bash\"stages.STAGE_ID.STEP_NAME.modules A list of kernel modules to load.\nstages:default:- name:\"Setup users\"modules:- nvidiastages.STAGE_ID.STEP_NAME.systemctl A list of systemd services to enable, disable, mask or start.\nstages:default:- name:\"Setup users\"systemctl:enable:- systemd-timesyncd- croniemask:- purge-kernelsdisable:- crondstart:- croniestages.STAGE_ID.STEP_NAME.environment A map of variables to write in /etc/environment, or otherwise specified in environment_file\nstages:default:- name:\"Setup users\"environment:FOO:\"bar\"stages.STAGE_ID.STEP_NAME.environment_file A string to specify where to set the environment file\nstages:default:- name:\"Setup users\"environment_file:\"/home/user/.envrc\"environment:FOO:\"bar\"stages.STAGE_ID.STEP_NAME.timesyncd Sets the systemd-timesyncd daemon file (/etc/system/timesyncd.conf) file accordingly. The documentation for timesyncd and all the options can be found here.\nstages:default:- name:\"Setup NTP\"systemctl:enable:- systemd-timesyncdtimesyncd:NTP:\"0.pool.org foo.pool.org\"FallbackNTP:\"\"...stages.STAGE_ID.STEP_NAME.commands A list of arbitrary commands to run after file writes and directory creation.\nstages:default:- name:\"Setup something\"commands:- echo 1 \u003e /barstages.STAGE_ID.STEP_NAME.datasource Sets to fetch user data from the specified cloud providers. It populates provider specific data into /run/config folder and the custom user data is stored into the provided path.\nstages:default:- name:\"Fetch cloud provider's user data\"datasource:providers:- \"aws\"- \"digitalocean\"path:\"/etc/cloud-data\"stages.STAGE_ID.STEP_NAME.layout Sets additional partitions on disk free space, if any, and/or expands the last partition. All sizes are expressed in MiB only and default value of size: 0 means all available free space in disk. This plugin is useful to be used in oem images where the default partitions might not suit the actual disk geometry.\nstages:default:- name:\"Repart disk\"layout:device:# It will partition a device including the given filesystem label# or partition label (filesystem label matches first) or the device# provided in 'path'. The label check has precedence over path when# both are provided.label:\"COS_RECOVERY\"path:\"/dev/sda\"# Only last partition can be expanded and it happens before any other# partition is added. size: 0 or unset means all available free spaceexpand_partition:size:4096add_partitions:- fsLabel:\"COS_STATE\"size:8192# No partition label is applied if omittedpLabel:\"state\"- fsLabel:\"COS_PERSISTENT\"# default filesystem is ext2 if omittedfilesystem:\"ext4\"","categories":"","description":"Features inherited by cOS derivatives that are also available in the cOS vanilla images\n","excerpt":"Features inherited by cOS derivatives that are also available in the …","ref":"/docs/reference/cloud_init/","tags":"","title":"Cloud-init support"},{"body":"This is a work in progress example of how to deploy K3S + Fleet + System Uprade Controller over a cOS vanilla image only by using cloud-init yaml configuration files. The config file reproduced here is meant to be included as a user-data in a cloud provider (aws, gcp, azure, etc) or as part of a cdrom (cOS-Recovery will try to fetch /userdata file from a cdrom device).\nA vanilla image is an image that only provides the cOS-Recovery system on a COS_RECOVERY partition. It does not include any other system and it is meant to be dumped to a bigger disk and deploy a cOS system or a derivative system over the free space in disk. COS vanilla images are build as part of the CI workflow, see CI artifacts to download one of those.\nThe configuration file of this example has two purposes: first it deploys cOS, second in reboots on the deployed OS and deploys K3S + Fleet + System Upgrades Controller.\nOn first boot it will fail to boot cOS grub menu entry and fallback to cOS-Recovery system. From there it will partition the vanilla image to create the main system partition (COS_STATE) and add an extra partition for persistent data (COS_PERSISTENT). It will use the full disk, a disk of at least 20GiB is recommended. After partitioning it will deploy the main system on COS_STATE and reboot to it.\nOn consequent boots it will simply boot from COS_STATE, there it prepares the persistent areas of the system (arranges few bind mounts inside COS_PERSISTENT) and then it runs an standard installation of K3s, Fleet and System Upgrade Controller. After few minutes after the system is up the K3s cluster is up and running.\nNote this setup similar to the derivative example using Fleet. The main difference is that this example does not require to build any image, it is pure cloud-init configuration based.\nUser data configuration file name:\"Default deployment\"stages:rootfs.after:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERYadd_partitions:- fsLabel:COS_STATE# 15Gb for COS_STATE, so the disk should have, at least, 20Gbsize:15360pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available spacepLabel:persistent- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Persistent state\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"RW_PATHS:\"/var /etc /srv\"PERSISTENT_STATE_PATHS:\"/root /opt /home /var/lib/rancher /var/lib/kubelet /etc/systemd /etc/rancher /etc/ssh\"network.before:- name:\"Setup SSH keys\"authorized_keys:root:# It can download ssh key from remote places, such as github user keys (e.g. `github:my_user`)- my_custom_ssh_key- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Fleet deployment\"files:- path:/etc/k3s/manifests/fleet-config.yamlcontent:|apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet-crd namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: fleet namespace: kube-system spec: chart: https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgznetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:# Deploys the latest image available in default channel (quay.io/costoolkit/releases-green)# use --docker-image to deploy a custom image# e.g. `cos-deploy --docker-image quay.io/my_custom_repo:my_image`- cos-deploy \u0026\u0026 shutdown -r now- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Setup k3s\"directories:- path:\"/usr/local/bin\"permissions:0755owner:0group:0commands:- |curl -sfL https://get.k3s.io | \\ INSTALL_K3S_VERSION=\"v1.20.4+k3s1\" \\ INSTALL_K3S_EXEC=\"--tls-san {{.Values.node.hostname}}\" \\ INSTALL_K3S_SELINUX_WARN=\"true\" \\ sh - # Install fleet kubectl apply -f /etc/k3s/manifests/fleet-config.yaml # Install system-upgrade-controller kubectl apply -f https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.6.2/manifests/system-upgrade-controller.yaml","categories":"","description":"Running k3s and Fleet on a cOS vanilla raw image\n","excerpt":"Running k3s and Fleet on a cOS vanilla raw image\n","ref":"/docs/tutorials/k3s_and_fleet_on_vanilla_image_example/","tags":"","title":"K3s + Fleet"},{"body":"You can create a sample policy by using audit2allow after running some basic operations in permissive mode using system default policies. allow2audit translates audit messages into allow/dontaudit SELinux policies which can be later compiled as a SELinux module. This is the approach used in this illustration example and mostly follows audit2allow man pages.\nOnce you have generated your policy (mypolicy.te) you can create a selinux package (create a new folder with build.yaml, definition.yaml and mypolicy.te) like so:\nbuild.yaml:\nrequires:- name:\"selinux-policies\"category:\"system\"version:\"\u003e=0\"env:- MODULE_NAME=mypolicysteps:- sed -i \"s|^SELINUX=.*|SELINUX=permissive|g\" /etc/selinux/config- rm -rf /.autorelabel- checkmodule -M -m -o ${MODULE_NAME}.mod ${MODULE_NAME}.te \u0026\u0026 semodule_package -o ${MODULE_NAME}.pp -m ${MODULE_NAME}.mod- semodule -i ${MODULE_NAME}.ppdefinition.yaml:\nname:\"policy\"category:\"selinux\"version:0.0.1Example mypolicy.te (generated with audit2alllow)\n#==== cOS SELinux targeted policy module ======== # # Disclaimer: This module is definition is for illustration use only. It # has no guarantees of completeness, accuracy and usefulness. It should # not be used \"as is\". # module epinioOS 1.0; require { type init_t; type audisp_t; type getty_t; type unconfined_t; type initrc_t; type bin_t; type tmpfs_t; type wicked_t; type systemd_logind_t; type sshd_t; type lib_t; type unlabeled_t; type chkpwd_t; type unconfined_service_t; type usr_t; type local_login_t; type cert_t; type system_dbusd_t; class lnk_file read; class file { execmod getattr open read }; class dir { getattr read search watch }; } #============= audisp_t ============== allow audisp_t tmpfs_t:lnk_file read; #============= chkpwd_t ============== allow chkpwd_t tmpfs_t:file { getattr open read }; #============= getty_t ============== allow getty_t tmpfs_t:file { getattr open read }; #============= init_t ============== allow init_t cert_t:dir watch; allow init_t usr_t:dir watch; #============= initrc_t ============== #!!!! This avc can be allowed using the boolean 'selinuxuser_execmod' allow initrc_t bin_t:file execmod; #============= local_login_t ============== allow local_login_t tmpfs_t:file { getattr open read }; allow local_login_t tmpfs_t:lnk_file read; #============= sshd_t ============== allow sshd_t tmpfs_t:lnk_file read; #============= system_dbusd_t ============== allow system_dbusd_t lib_t:dir watch; allow system_dbusd_t tmpfs_t:lnk_file read; #============= systemd_logind_t ============== allow systemd_logind_t unlabeled_t:dir { getattr search }; #============= unconfined_service_t ============== #!!!! This avc can be allowed using the boolean 'selinuxuser_execmod' allow unconfined_service_t bin_t:file execmod; #============= unconfined_t ============== #!!!! This avc can be allowed using the boolean 'selinuxuser_execmod' allow unconfined_t bin_t:file execmod; #============= wicked_t ============== allow wicked_t tmpfs_t:dir read; allow wicked_t tmpfs_t:file { getattr open read }; allow wicked_t tmpfs_t:lnk_file read; ","categories":"","description":"Build SELinux policies with cOS\n","excerpt":"Build SELinux policies with cOS\n","ref":"/docs/reference/selinux/","tags":"","title":"SELinux"},{"body":"In this tutorial we will:\n Build a custom OS image to deploy in our cluster Setup a cluster with cOS, k3s and fleet Upgrade the cluster to our custom OS image with fleet  This repository contains the full example code.\n1) Build the OS image # IMAGE=quay.io/costoolkit/test-images:fleet-sample # cd os # docker build -t $IMAGE . 2) Push the docker image # docker push $IMAGE 3) Prepare a cOS VM Download an ISO, or a qcow image from the Github artifacts of cOS. Or generate an iso of the image (check here for another example).\nIf deploying on AWS/openstack/Cloud, use the fleet-cloud-init.yaml file as userdata. If deploying on baremetal/VMs, place fleet-cloud-init.yaml in /oem after install (or run the installer with elemental install --cloud-init https://raw.githubusercontent.com/rancher-sandbox/cos-fleet-upgrades-sample/main/fleet-cloud-init.yaml $DEVICE).\nReboot, after some bootstraping time (check until all pods are running with watch kubectl get pods -A), you should have a k3s cluster with fleet and system-upgrade-controller deployed.\n4) Upgrade with fleet Add your fleet repository to the fleet cluster:\ncat \u003e example.yaml \u003c\u003c \"EOF\" apiVersion: fleet.cattle.io/v1alpha1 kind: GitRepo metadata: name: upgrade # This namespace is special and auto-wired to deploy to the local cluster namespace: fleet-local spec: # Everything from this repo will be ran in this cluster. You trust me right? repo: \"https://github.com/rancher-sandbox/cos-fleet-upgrades-sample\" branch: \"main\" paths: - manifests EOF kubectl apply -f example.yaml An example of how to trigger an upgrade with fleet is in manifests/upgrade.yaml. Edit the image with the one generated in the previous steps, and commit it to your fleet repository, At this point you should see the upgrade job to kick-in, the system will reboot afterwards.\n","categories":"","description":"Using fleet to trigger upgradeson cOS based derivatives\n","excerpt":"Using fleet to trigger upgradeson cOS based derivatives\n","ref":"/docs/tutorials/trigger_upgrades_with_fleet/","tags":"","title":"Trigger upgrades with K3s and Fleet"},{"body":"You can find the examples below in the examples folder.\nFrom standard images Besides using the cos-toolkit toolchain, it’s possible to create standard container images which are consumable by the vanilla cOS images (ISO, Cloud Images, etc.) during the upgrade and deploy phase.\nAn example of a Dockerfile image can be:\nARG LUET_VERSION=0.20.6FROMquay.io/luet/base:$LUET_VERSION AS luetFROMopensuse/leap:15.3ENV COSIGN_EXPERIMENTAL=1 ENV COSIGN_REPOSITORY=raccos/releases-greenARG ARCH=amd64 ENV ARCH=${ARCH}RUN zypper in -y \\  bash-completion \\  conntrack-tools \\  coreutils \\  curl \\  device-mapper \\  dosfstools \\  dracut \\  e2fsprogs \\  findutils \\  gawk \\  gptfdisk \\  grub2-i386-pc \\  grub2-x86_64-efi \\  haveged \\  iproute2 \\  iptables \\  iputils \\  issue-generator \\  jq \\  kernel-default \\  kernel-firmware-bnx2 \\  kernel-firmware-i915 \\  kernel-firmware-intel \\  kernel-firmware-iwlwifi \\  kernel-firmware-mellanox \\  kernel-firmware-network \\  kernel-firmware-platform \\  kernel-firmware-realtek \\  less \\  lsscsi \\  lvm2 \\  mdadm \\  multipath-tools \\  nano \\  nfs-utils \\  open-iscsi \\  open-vm-tools \\  parted \\  pigz \\  policycoreutils \\  procps \\  python-azure-agent \\  qemu-guest-agent \\  rng-tools \\  rsync \\  squashfs \\  strace \\  systemd \\  systemd-sysvinit \\  tar \\  timezone \\  vim \\  which# Copy the luet config file pointing to the upgrade repositoryCOPY conf/luet.yaml /etc/luet/luet.yaml# Copy luet from the official imagesCOPY --from=luet /usr/bin/luet /usr/bin/luetRUN luet install -y meta/cos-verifyRUN luet install --plugin luet-cosign -y meta/cos-minimal \\  utils/k9s \\  utils/nerdctlCOPY files/ /RUN mkinitrd   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/standard/Dockerfile   While the config file:\nlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"cos\"description:\"cOS official\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-green\"   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/standard/conf/luet.yaml   We can just run docker to build the image with\ndocker build -t $IMAGE . The important piece is that an image needs to ship at least:\ntoolchain/yip utils/installer system/cos-setup system/immutable-rootfs system/grub2-config from the toolchain. If you want to customize further the container further add more step afterwards luet install see the customizing section.\nNote Depending on the base image (FROM opensuse/leap:15.3 in the sample), you must set the corresponding repository for each flavor see releases in the luet config file ( which in the sample above points to the green releases )  Generating from CI image Derivatives can be stacked on top of another, so it is possible to reuse directly also the vanilla cOS images:\n# Pick one version from https://quay.io/repository/costoolkit/releases-green?tab=tagsFROMquay.io/costoolkit/releases-green:cos-system-0.5.8-4COPY files/ /   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/cos-official/Dockerfile   The images contains already the toolkit, so they can be used as-is and apply further customization on top.\nFrom scratch The luet image quay.io/luet/base contains just luet, and can be used to boostrap the base system from scratch:\nconf/luet.yaml: logging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"cos\"description:\"cOS official\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-green\"   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/scratch/conf/luet.yaml   Dockerfile: ARG LUET_VERSION=0.20.6FROMquay.io/luet/base:$LUET_VERSION AS luetFROMopensuse/leap:15.3 AS caFROMscratch# Copy luet from the official imagesCOPY --from=luet /usr/bin/luet /usr/bin/luetCOPY --from=ca /etc/ssl/certs/. /etc/ssl/certs/# Copy the luet config file pointing to the cOS repositoryADD conf/luet.yaml /etc/luet/luet.yamlENV USER=root ENV LUET_NOLOCK=true SHELL [\"/usr/bin/luet\", \"install\", \"-y\", \"-d\"]RUN system/cos-containerSHELL [\"/bin/sh\", \"-c\"]RUN rm -rf /var/cache/luet/packages/ /var/cache/luet/repos/ENV TMPDIR=/tmpENTRYPOINT [\"/bin/sh\"]   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/scratch/Dockerfile   Customizations All the method above imply that the image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image:\n Everything under /system/oem will be loaded during the various stage (boot, network, initramfs). You can check here for the cOS defaults. See 00_rootfs.yaml to customize the booting layout. /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB /etc/cos-upgrade-image contains the default upgrade configuration for recovery and the booting system image  Configuration file The example configuration file shows how to enable the cos-toolkit repository:\nlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"cos\"description:\"cOS official\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-green\"   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/standard/conf/luet.yaml   Repositories have the following fields, notably:\n name: Repository name enable: Enable/disables the repository arch: (optional) Denotes the arch repository. If present, it will enable the repository automatically if the corresponding arch is matching with the host running luet. enable: true would override this behavior reference: (optional) A reference to a repository index file to use to retrieve the repository metadata instead of latest. This can be used to point to a different or an older repository index to act as a “wayback machine”. The client will consume the repository state from that snapshot instead of latest.  Note The reference field has to be a valid tag. For example, for the green flavor, browse the relevant container image list page. The repository index snapshots are prefixed with a timestamp, and ending in repository.yaml. For example  20211027153653-repository.yaml  ","categories":"","description":"This document describes the requirements to create standard container images that can be used for `cOS` deployments\n","excerpt":"This document describes the requirements to create standard container …","ref":"/docs/examples/creating_bootable_images/","tags":"","title":"Creating bootable images"},{"body":"cos-toolkit is a manifest to share a common abstract layer between derivatives inheriting the same featureset.\ncos is a Luet tree and derivatives can be expressed as Luet trees as well that inherit part of the compilation specs from cos.\nThose trees are then post-processed and converted to Dockerfiles when building packages, that in turn are used to build docker images and final artefacts.\nHigh level workflow The building workflow can be resumed in the following steps:\n Build packages from container images. This step generates build metadata (luet build / docker build / buildah ..) Add repository metadata and create a repository from the build phase (luet create-repo) (otherwise, optionally) publish the repository and the artefacts along (luet create-repo --push-images)  While on the client side, the upgrade workflow is:\n luet install (when upgrading from release channels) latest cos on a pristine image file or luet util unpack (when upgrading from specific docker images)  Note: The manual build steps are not stable and will likely change until we build a single CLI to encompass the cos-toolkit components, rather use source .envrc \u0026\u0026 cos-build for the moment being while iterating locally.\nSingle image OS Derivatives are composed by a combination of specs to form a final package that is consumed as a single image OS.\nThe container image during installation and upgrade, is converted to an image file with a backing ext2 fs.\nPackages in luet have runtime and buildtime specifications into definition.yaml and build.yaml respectively, and in the buildtime we set:\nrequires:- category:\"system\"name:\"cos\"version:\"\u003e=0\"- category:\"app\"name:\"sampleOSService\"version:\"\u003e=0\"This instruct luet to compose a new image from the results of the compilation of the specified packages, without any version constraints, and use it to run any steps and prelude on top of it.\nWe are interested in the dependencies final images, and not the containers used to build them, so we enable requires_final_images:\nrequires_final_images: true We later run arbitrary steps to tweak the image:\nsteps:- ...And we instruct luet to compose the final artifact as a squash of the resulting container image, composed of all the files:\nunpack:trueA detailed explaination of all the keywords available is in the luet docs along with the supported build strategies.\nWe exclude then a bunch of file that we don’t want to be in the final package (regexp supported):\nexcludes:- ..Templating The package build definition supports templating, and global interpolation of build files with multiple values files.\nValues file can be specified during build time in luet with the --values flag (also multiple files are allowed) and, if you are familiar with helm it using the same engine under the hood, so all the functions are available as well.\ncos-toolkit itself uses default values files for every supported distributions.\nTemplates uses cases are for: resharing common pieces between flavors, building for different platforms and architectures, …\nBuild ISO To build an iso from a local repository (the build process, automatically produces a repository in build in the local checkout):\nluet-makeiso ./iso.yaml --local build Where iso.yaml is the iso specification file, and --local build is an optional argument to use also the local repository in the build process. See also building ISOs\n","categories":"","description":"This document summarize references to create derivatives with `cos-toolkit` by using the `luet` toolchain.\n","excerpt":"This document summarize references to create derivatives with …","ref":"/docs/development/creating_derivatives/","tags":"","title":"Creating derivatives"},{"body":"This guide will guide in a step-by-step process to build a derivative which is fully compatible with cOS, and will illustrate how to make customization on such image, by adding for example a default set of services and a custom user.\nThe derivative will be based on openSUSE and embed k3s, and a custom user joe which will be already set to allow us to login.\nPrerequisites  Docker installed  1) Create a Dockerfile Let’s create a workspace directory and move into it:\n$\u003e mkdir derivative $\u003e cd derivative Let’s create now a Dockerfile for our image inside that directory, which will be represent running system:\n# Let's copy over luet from official images. # This version will be used to bootstrap luet itself and cOS internal componentsARG LUET_VERSION=0.16.7FROMquay.io/luet/base:$LUET_VERSION AS luetFROMopensuse/leap:15.3ARG K3S_VERSION=v1.20.4+k3s1ARG ARCH=amd64 ENV ARCH=${ARCH}# Install packages from the base imageRUN zypper in -y \\  bash-completion \\  conntrack-tools \\  coreutils \\  curl \\  device-mapper \\  dosfstools \\  dracut \\  e2fsprogs \\  findutils \\  gawk \\  gptfdisk \\  grub2-i386-pc \\  grub2-x86_64-efi \\  haveged \\  iproute2 \\  iptables \\  iputils \\  issue-generator \\  jq \\  kernel-default \\  kernel-firmware-bnx2 \\  kernel-firmware-i915 \\  kernel-firmware-intel \\  kernel-firmware-iwlwifi \\  kernel-firmware-mellanox \\  kernel-firmware-network \\  kernel-firmware-platform \\  kernel-firmware-realtek \\  less \\  lsscsi \\  lvm2 \\  mdadm \\  multipath-tools \\  nano \\  nfs-utils \\  open-iscsi \\  open-vm-tools \\  parted \\  pigz \\  policycoreutils \\  procps \\  python-azure-agent \\  qemu-guest-agent \\  rng-tools \\  rsync \\  squashfs \\  strace \\  systemd \\  systemd-sysvinit \\  tar \\  timezone \\  vim \\  which# Copy the luet config file pointing to the upgrade repositoryCOPY repositories.yaml /etc/luet/luet.yaml# Copy luet from the official imagesCOPY --from=luet /usr/bin/luet /usr/bin/luetRUN luet install -y \\  toolchain/yip \\  toolchain/luet \\  utils/installer \\  system/cos-setup \\  system/immutable-rootfs \\  system/grub2-config \\  system/base-dracut-modules# Install k3s server/agentENV INSTALL_K3S_VERSION=${K3S_VERSION}RUN curl -sfL https://get.k3s.io \u003e installer.sh \u0026\u0026 \\  INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh \u0026\u0026 \\  INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh agent \u0026\u0026 \\  rm -rf installer.sh## System layout# Required by k3s etc.RUN mkdir /usr/libexec \u0026\u0026 touch /usr/libexec/.keep# Copy custom files# COPY files/ /# Copy cloud-init default configurationCOPY cloud-init.yaml /system/oem/# Generate initrdRUN mkinitrd# OS level configurationRUN echo \"VERSION=999\" \u003e /etc/os-releaseRUN echo \"GRUB_ENTRY_NAME=derivative\" \u003e\u003e /etc/os-releaseRUN echo \"welcome to our derivative\" \u003e\u003e /etc/issue.d/01-derivative# Copy cloud-init default configurationCOPY cloud-init.yaml /system/oem/Create then repositories.yaml in derivative/repositories.yaml with the following content:\nlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"cos\"description:\"cOS official\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-green\"   Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/examples/standard/conf/luet.yaml   2) Configuration At the end of the Dockerfile, you can see that we copy over a custom cloud-init file:\n# Copy cloud-init default configurationCOPY cloud-init.yaml /system/oem/Create a cloud-init.yaml file as the derivative/cloud-init.yaml with the following content:\n# See https://rancher-sandbox.github.io/cos-toolkit-docs/docs/reference/cloud_init/ for a full syntax referencename:\"Default settings\"stages:initramfs:# Setup default hostname- name:\"Branding\"hostname:\"derivative\"# Setup an admin group with sudo access- name:\"Setup groups\"ensure_entities:- entity:|kind: \"group\" group_name: \"admin\" password: \"x\" gid: 900# Setup network - openSUSE specific- name:\"Network setup\"files:- path:/etc/sysconfig/network/ifcfg-eth0content:|BOOTPROTO='dhcp' STARTMODE='onboot'permissions:0600owner:0group:0# Setup a custom user- name:\"Setup users\"users:# Replace the default user name here and settingsjoe:# Comment passwd for no passwordpasswd:\"joe\"shell:/bin/bashhomedir:\"/home/joe\"groups:- \"admin\"#authorized_keys:# Replace here with your ssh keys# joe: # - ssh-rsa ....# Setup sudo- name:\"Setup sudo\"files:- path:\"/etc/sudoers\"owner:0group:0permsisions:0600content:|Defaults always_set_home Defaults secure_path=\"/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin\" Defaults env_reset Defaults env_keep = \"LANG LC_ADDRESS LC_CTYPE LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE LC_ATIME LC_ALL LANGUAGE LINGUAS XDG_SESSION_COOKIE\" Defaults !insults root ALL=(ALL) ALL %admin ALL=(ALL) NOPASSWD: ALL @includedir /etc/sudoers.dcommands:- passwd -l root# Setup persistency so k3s works properly# See also: https://rancher-sandbox.github.io/cos-toolkit-docs/docs/reference/immutable_rootfs/#configuration-with-an-environment-filerootfs.after:- name:\"Immutable Layout configuration\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"RW_PATHS:\"/var /etc /srv\"PERSISTENT_STATE_PATHS:\u003e-/etc/systemd /etc/rancher /etc/ssh /etc/iscsi /etc/cni /home /opt /root /usr/libexec /var/log /var/lib/rancher /var/lib/kubelet /var/lib/wicked /var/lib/longhorn /var/lib/cniPERSISTENT_STATE_BIND:\"true\"# Finally, let's start k3s when network is available, and download the SSH key from github for the joe usernetwork:- name:\"Start k3s\"systemctl:start:- k3s- authorized_keys:# Replace here with your ssh keys or github handlejoe:- github:mudlerDone! We are now ready to build the Docker container.\nThe file structure should be like the following:\n$\u003e tree ./derivative derivative ├── cloud-init.yaml ├── Dockerfile ├── iso.yaml └── repositories.yaml 3) Build it! Now we are ready to build our docker image:\n$~/derivative\u003e docker build -t derivative:latest . ... ---\u003e Running in a9c33b42f567 Removing intermediate container a9c33b42f567 ---\u003e 8e83191d29df Step 19/19 : COPY cloud-init.yaml /system/oem/ ---\u003e 38cc4c8b173a Successfully built 38cc4c8b173a Successfully tagged derivative:latest After the process completed, we are ready to consume our docker image. If you push the image over a container registry, you can then or use a running cOS system to upgrade to it, or deploy it directly see getting started.\nBuild an ISO image We can at this point also create a ISO from it.\nCreate a iso.yaml file with the following content inside the derivative folder:\npackages:uefi:- live/grub2-efi-imageisoimage:- live/grub2- live/grub2-efi-imageboot_file:\"boot/x86_64/loader/eltorito.img\"boot_catalog:\"boot/x86_64/boot.catalog\"isohybrid_mbr:\"boot/x86_64/loader/boot_hybrid.img\"initramfs:kernel_file:\"vmlinuz\"rootfs_file:\"initrd\"image_prefix:\"derivative-0.\"image_date:truelabel:\"COS_LIVE\"luet:repositories:- name:cOSenable:trueurls:- quay.io/costoolkit/releases-greentype:dockerNow, we can build the ISO with:\n$~/derivative\u003e docker run -v $PWD:/cOS -v /var/run:/var/run --entrypoint /usr/bin/luet-makeiso -ti --rm quay.io/costoolkit/toolchain ./iso.yaml --image derivative:latest .... INFO[0114] Copying BIOS kernels INFO[0114] Create squashfs Parallel mksquashfs: Using 8 processors Creating 4.0 filesystem on /tmp/luet-geniso4082786464/tempISO/rootfs.squashfs, block size 1048576. .... INFO[0247] 🍹 Generate ISO derivative-0.20210909.iso xorriso 1.4.6 : RockRidge filesystem manipulator, libburnia project. Drive current: -outdev 'stdio:derivative-0.20210909.iso' Media current: stdio file, overwriteable Media status : is blank Media summary: 0 sessions, 0 data blocks, 0 data, 448g free Added to ISO image: directory '/'='/tmp/luet-geniso4082786464/tempISO' xorriso : UPDATE : 599 files added in 1 seconds xorriso : UPDATE : 599 files added in 1 seconds xorriso : NOTE : Copying to System Area: 512 bytes from file '/tmp/luet-geniso4082786464/tempISO/boot/x86_64/loader/boot_hybrid.img' xorriso : WARNING : Boot image load size exceeds 65535 blocks of 512 bytes. Will record 0 in El Torito to extend ESP to end-of-medium. libisofs: NOTE : Aligned image size to cylinder size by 137 blocks xorriso : UPDATE : 12.35% done xorriso : UPDATE : 42.73% done ISO image produced: 282624 sectors Written to medium : 282624 sectors at LBA 0 Writing to 'stdio:derivative-0.20210909.iso' completed successfully. After the process completes, we should have a ISO in our folder ready to be used. See the build ISOs section for all the available options.\nCustomization Here follows a break down of the steps above\nAdding packages Feel free to edit the Dockerfile with the packages you want to embed in our image. You can install any packages available in the openSUSE repositories by tweaking\n# Install packages from the base imageRUN zypper in -y \\  .... # Add more packages here!Repositories configuration We copy the configuration file of luet which just points to cOS repositories with:\n# Copy the luet config file pointing to the upgrade repository COPY repositories.yaml /etc/luet/luet.yaml The config file should point to the green flavor as our derivative will be based on opensuse. It should point to blue, or orange instead if building against Fedora or Ubuntu.\ncOS toolkit We install packages from the cOS toolkit with luet. In this case we pick the basic ones that allows us to install/upgrade immutable cOS derivatives:\nRUN luet install -y \\  toolchain/yip \\  toolchain/luet \\  utils/installer \\  system/cos-setup \\  system/immutable-rootfs \\  system/grub2-config \\  system/base-dracut-modulesYou can check all the available packages here, and you can learn more about this in our Creating Derivatives section.\nSystem layout and k3s We set some default layouts and install k3s:\n# Install k3s server/agent ENV INSTALL_K3S_VERSION=${K3S_VERSION} RUN curl -sfL https://get.k3s.io \u003e installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh \u0026\u0026 \\ INSTALL_K3S_SKIP_START=\"true\" INSTALL_K3S_SKIP_ENABLE=\"true\" sh installer.sh agent \u0026\u0026 \\ rm -rf installer.sh ## System layout # Required by k3s etc. RUN mkdir /usr/libexec \u0026\u0026 touch /usr/libexec/.keep # Copy custom files # COPY files/ / # Generate initrd RUN mkinitrd # OS level configuration RUN echo \"VERSION=999\" \u003e /etc/os-release RUN echo \"GRUB_ENTRY_NAME=derivative\" \u003e\u003e /etc/os-release RUN echo \"welcome to our derivative\" \u003e\u003e /etc/issue.d/01-derivative As our target here is to install k3s we do install both k3s agent and server, so the image can work in both modes. Here we could have installed any service or binary that we want to embed in our container image. We setup the system layout by creating needed paths for k3s and set up a os-release which identifies the OS version. Afterward we regenerate the initrd which is required in order to boot, see also the Initrd section.\nCloud-init, custom SSH access The cloud-init.yaml file above configures a user, joe and attaches a ssh key to it in order to login. It also sets up a default password, which is optional.\nTo specify any additional ssh key installed within the user, we do:\nnetwork:- authorized_keys:joe:- github:mudlerwhich you can replace with your github handle, or by specifying directly an ssh key. In case you specify the SSH key directly, you don’t need to run the step in the network stage.\nThe user will be part of the admin group which is allowed to use sudo.\nAs our target is to run k3s, but could have been any other service, we tweak the immutable setup by specifying sensible path required for k3s in order to function properly, see immutable rootfs for all the available options.\nFinally, we start k3s. Note, we could have tweaked that part slightly to provide k3s configurations via systemd env files, or boot up for example the agent instead of the server:\nnetwork:- if:'[ ! -f \"/run/cos/recovery_mode\" ]'name:\"Setup k3s\"# Setup environment file for custom k3s argumentsenvironment_file:\"/etc/systemd/system/k3s.service.env\"environment:FOO:\"bar\"systemctl:start:- k3s- commands:- |chmod 600 /etc/systemd/system/k3s.service.envReferences You can find the complete example here where this article was made from.\n","categories":"","description":"This document is a step-by-step guide to build a customized embedded system that can be used in cOS\n","excerpt":"This document is a step-by-step guide to build a customized embedded …","ref":"/docs/examples/embedded_images/","tags":"","title":"Creating embedded images"},{"body":"","categories":"","description":"This section contains various articles relative on how to customize cOS, branding and behavior.\n","excerpt":"This section contains various articles relative on how to customize …","ref":"/docs/customizing/","tags":"","title":"Customizing"},{"body":"cOS during installation, reset and upgrade (elemental install, elemental reset and elemental upgrade respectively) will read a configuration file in order to apply derivative customizations. The configuration files are sourced in precedence order and can be located in the following places:\n /etc/environment /etc/os-release /etc/cos/config  Below you can find a full example of the config file:\n# cOS configuration file # This file allows to tweak cOS configuration such as: default upgrade/recovery image and GRUB menu entry # Disable/enable image verification during upgrades ( default: true ) #VERIFY=false # Disable/enable upgrades via release channels instead of container images. ( default: true ) #CHANNEL_UPGRADES=false # Default container image used for upgrades. ( defaults to system/cos with channel CHANNEL_UPGRADES enabled ) #UPGRADE_IMAGE=\"quay.io/mudler/cos-test:cos-standard\" # Default recovery image to use when upgrading the recovery partition # ( defaults to recovery/cos in vanilla cOS images with channel CHANNEL_UPGRADES enabled. Otherwise it defaults to UPGRADE_IMAGE ). #RECOVERY_IMAGE=\"quay.io/mudler/cos-test:cos-standard\" # GRUB entry to display on boot. ( defaults: cOS ) #GRUB_ENTRY_NAME=\"example\" # Space separated list of additional paths that are used to # source cloud-config from. ( defaults paths are: /system/oem /oem/ /usr/local/cloud-config/ ) #CLOUD_INIT_PATHS=\"\" # This is the directory that can be used to store cloud-init files that can be enabled/disabled in runtime # by cos-features. ( defaults to /system/features ) #COS_FEATURESDIR=\"/system/features\" # This is the repository that hosts the signature files used by cosign and luet-cosign plugin during upgrade/deploy to # check the artifact signatures #COSIGN_REPOSITORY=\"raccos/releases-green\" # This sets keyless verify on building packages with luet and the luet-cosign plugin. # 1 = enabled keyless, 0 = disabled, uses normal public key verification #COSIGN_EXPERIMENTAL=1 # This sets the location of the public key to use to verify the packages installed by luet during upgrade/deploy # This can be set to file, URL, KMS URI or Kubernetes Secret # This is only used if COSIGN_EXPERIMENTAL is set to 0 #COSIGN_PUBLIC_KEY_LOCATION=\"\" # Default size (in MB) of disk image files (.img) created during upgrades #DEFAULT_IMAGE_SIZE=3240 # Path to default configuration grub file #GRUBCONF=\"/etc/cos/grub.cfg\" # Label of the OEM volume OEM_LABEL=\"COS_OEM\" # Label of the PERSISTENT volume PERSISTENT_LABEL=\"COS_PERSISTENT\" # Label of the RECOVERY volume RECOVERY_LABEL=\"COS_RECOVERY\" # Label of the STATE volume STATE_LABEL=\"COS_STATE\" # Label of the ACTIVE volume ACTIVE_LABEL=\"COS_ACTIVE\" # Label of the PASSIVE volume PASSIVE_LABEL=\"COS_PASSIVE\" # Label of the SYSTEM volume SYSTEM_LABEL=\"COS_SYSTEM\"    Complete source code: https://github.com/rancher-sandbox/cos-toolkit/blob/master/packages/cos-config/cos-config   ","categories":"","description":"Configuring a cOS derivative\n","excerpt":"Configuring a cOS derivative\n","ref":"/docs/customizing/general_configuration/","tags":"","title":"General Configuration"},{"body":"When building cOS or a cOS derivative, you could face different issues, this section provides a description of the most known ones, and way to workaround them.\nBuilding SELinux fails cOS by default has SELinux enabled in permissive mode. If you are building parts of cOS or cOS itself from scratch, you might encounter issues while building the SELinux module, like so:\nStep 12/13 : RUN checkmodule -M -m -o cOS.mod cOS.te \u0026\u0026 semodule_package -o cOS.pp -m cOS.mod ---\u003e Using cache ---\u003e 1be520969ead Step 13/13 : RUN semodule -i cOS.pp ---\u003e Running in c5bfa5ae92e2 libsemanage.semanage_commit_sandbox: Error while renaming /var/lib/selinux/targeted/active to /var/lib/selinux/targeted/previous. (Invalid cross-device link). semodule: Failed! The command '/bin/sh -c semodule -i cOS.pp' returned a non-zero code: 1 Error: while resolving join images: failed building join image: Failed compiling system/selinux-policies-0.0.6+3: failed building package image: Could not push image: raccos/sampleos:ffc8618ecbfbffc11cc3bca301cc49867eb7dccb623f951dd92caa10ced29b68 selinux-policies-system-0.0.6+3.dockerfile: Could not build image: raccos/sampleos:ffc8618ecbfbffc11cc3bca301cc49867eb7dccb623f951dd92caa10ced29b68 selinux-policies-system-0.0.6+3.dockerfile: Failed running command: : exit status 1 Bailing out make: *** [Makefile:45: build] Error 1 The issue is possibly caused by https://github.com/docker/for-linux/issues/480 . A workaround is to switch the storage driver of Docker. Check if your storage driver is overlay2, and switch it to devicemapper\nMulti-stage copy build fails While processing images with several stage copy, you could face the following:\n 🐋 Building image raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d done 📦 8/8 system/cos-0.5.3+1 ⤑ 🔨 build system/selinux-policies-0.0.6+3 ✅ Done 🚀 All dependencies are satisfied, building package requested by the user system/cos-0.5.3+1 📦 system/cos-0.5.3+1 Using image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d 📦 system/cos-0.5.3+1 🐋 Generating 'builder' image from raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d as raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed with prelude steps 🚧 warning Failed to download 'raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed'. Will keep going and build the image unless you use --fatal 🚧 warning Failed pulling image: Error response from daemon: manifest for raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed not found: manifest unknown: manifest unknown : exit status 1 🐋 Building image raccos/sampleos:builder-8533d659df2505a518860bd010b7a8ed Sending build context to Docker daemon 9.728kB Step 1/10 : FROM raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d ---\u003e f1122e79b17e Step 2/10 : COPY . /luetbuild ---\u003e 4ff3e202951b Step 3/10 : WORKDIR /luetbuild ---\u003e Running in 7ec571b96c6f Removing intermediate container 7ec571b96c6f ---\u003e 9e05366f830a Step 4/10 : ENV PACKAGE_NAME=cos ---\u003e Running in 30297dbd21a3 Removing intermediate container 30297dbd21a3 ---\u003e 4c4838b629f4 Step 5/10 : ENV PACKAGE_VERSION=0.5.3+1 ---\u003e Running in 36361b617252 Removing intermediate container 36361b617252 ---\u003e 6ac0d3a2ff9a Step 6/10 : ENV PACKAGE_CATEGORY=system ---\u003e Running in f20c2cf3cf34 Removing intermediate container f20c2cf3cf34 ---\u003e a902ff95d273 Step 7/10 : COPY --from=quay.io/costoolkit/build-cache:f3a333095d9915dc17d7f0f5629a638a7571a01dcf84886b48c7b2e5289a668a /usr/bin/yip /usr/bin/yip ---\u003e 42fa00d9c990 Step 8/10 : COPY --from=quay.io/costoolkit/build-cache:e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1 /usr/bin/luet /usr/bin/luet e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1: Pulling from costoolkit/build-cache 3599716b36e7: Already exists 24a39c0e5d06: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 378615c429f5: Already exists c28da22d3dfd: Already exists ddb4dd5c81b0: Already exists 92db41c0c9ab: Already exists 4f4fb700ef54: Already exists 6e0ca71a6514: Already exists 47debb886c7d: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists 4f4fb700ef54: Already exists d0c9d0f8ddb6: Already exists e5a48f1f72ad: Pulling fs layer 4f4fb700ef54: Pulling fs layer 7d603b2e4a37: Pulling fs layer 64c4d787e344: Pulling fs layer f8835d2e60d1: Pulling fs layer 64c4d787e344: Waiting f8835d2e60d1: Waiting e5a48f1f72ad: Download complete e5a48f1f72ad: Pull complete 4f4fb700ef54: Verifying Checksum 4f4fb700ef54: Download complete 4f4fb700ef54: Pull complete 7d603b2e4a37: Verifying Checksum 7d603b2e4a37: Download complete 64c4d787e344: Verifying Checksum 64c4d787e344: Download complete 7d603b2e4a37: Pull complete 64c4d787e344: Pull complete f8835d2e60d1: Verifying Checksum f8835d2e60d1: Download complete f8835d2e60d1: Pull complete Digest: sha256:9b58bed47ff53f2d6cc517a21449cae686db387d171099a4a3145c8a47e6a1e0 Status: Downloaded newer image for quay.io/costoolkit/build-cache:e3bbe48c6d57b93599e592c5540ee4ca7916158461773916ce71ef72f30abdd1 failed to export image: failed to create image: failed to get layer sha256:118537d8997a08750ab1ac3d8e8575e40fe60e8337e02633b0d8a1287117fe78: layer does not exist Error: while resolving join images: failed building join image: failed building package image: Could not push image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d cos-system-0.5.3+1-builder.dockerfile: Could not build image: raccos/sampleos:cc0aee4ff6c194f920a945c45ebcb487c3e22c5ab40e2634ea70c064dfab206d cos-system-0.5.3+1-builder.dockerfile: Failed running command: : exit status 1 Bailing out make: *** [Makefile:45: build] Error 1 There is a issue open upstream about it. A workaround is to enable Docker buildkit with DOCKER_BUILDKIT=1.\n","categories":"","description":"This document encompasses known issues while building cOS and cOS derivatives.\n","excerpt":"This document encompasses known issues while building cOS and cOS …","ref":"/docs/reference/known_issues/","tags":"","title":"Known issues"},{"body":"By default you can login with the user root and cos in a vanilla cOS image, this is also set automatically by the  system/cloud-config  package if used by a derivative.\nYou can change this by overriding /system/oem/04_accounting.yaml in the container image if present, or via cloud-init.\nExamples  Example accounting file  ","categories":"","description":"Default login, and how to override it\n","excerpt":"Default login, and how to override it\n","ref":"/docs/customizing/login/","tags":"","title":"Login"},{"body":"There are several way to customize cOS and a cos-toolkit derivative:\n declaratively in runtime with cloud-config file (by overriding, or extending) stateful, embedding any configuration in the container image to be booted.  For runtime persistence configuration, the only supported way is with cloud-config files, see the relevant docs.\nA derivative automatically loads and executes cloud-config files during the various system stages also inside /system/oem which is read-only and reserved to the system.\nNote The cloud-config mechanism works also as an emitter event pattern - running services or programs can emit new custom stages in runtime by running cos-setup stage_name.  Derivatives that wish to override default configurations can do that by placing extra cloud-init file, or overriding completely /system/oem in the target image.\nThis is to setup for example, the default root password or the prefered upgrade channel.\nThe following are the cOS default oem files, which are shipped within the  system/cloud-config  , which is an optional package:\n/system/oem/00_rootfs.yaml - defines the rootfs mountpoint layout setting /system/oem/01_defaults.yaml - systemd defaults (keyboard layout, timezone) /system/oem/02_upgrades.yaml - Settings for cOS vanilla channel upgrades /system/oem/03_branding.yaml - Branding setting, Derivative name, /etc/issue content /system/oem/04_accounting.yaml - Default user/pass /system/oem/05_network.yaml - Default network setup /system/oem/06_recovery.yaml - Executes additional commands when booting in recovery mode You can either override the above files, or alternatively not consume the  system/cloud-config  package while building a derivative.\n","categories":"","description":"OEM configuration reserved to cOS and derivatives\n","excerpt":"OEM configuration reserved to cOS and derivatives\n","ref":"/docs/customizing/oem_configuration/","tags":"","title":"OEM configuration"},{"body":"cOS vanilla images by default are picking upgrades by the standard upgrade channel. It means it will always get the latest published cOS version by our CI.\nHowever, it’s possible to tweak the default behavior of elemental upgrade to point to a specific docker image/tag, or a different release channel.\nBy default, cos derivatives if not specified will point to latest cos-toolkit. To override, you need to or overwrite the content of /system/oem/02_upgrades.yaml or supply an additional one, e.g. /system/oem/03_upgrades.yaml in the final image, see the default here.\nConfiguration elemental upgrade during start reads the cOS configuration file and allows to tweak the following:\n# Tweak the package to upgrade to, or the docker image (full reference) ELEMENTAL_UPGRADE_IMAGE=system/cos # Turn on/off channel upgrades. If disabled, UPGRADE_IMAGE should be a full reference to a container image ELEMENTAL_CHANNEL_UPGRADES=true # Disable mtree verification. Enabled by default ELEMENTAL_NO_VERIFY=true # Specify a separate recovery image (defaults to UPGRADE_IMAGE) ELEMENTAL_RECOVERY_IMAGE=recovery/cos elemental upgrade also reads its configuration from /etc/cos-upgrade-image if the file is present in the system.\nSpecifically, it allows to configure:\n ELEMENTAL_UPGRADE_IMAGE: A container image reference ( e.g. registry.io/org/image:tag ) or a luet package ( e.g. system/cos ) ELEMENTAL_CHANNEL_UPGRADES: Boolean indicating wether to use channel upgrades or not. If it is disabled UPGRADE_IMAGE should refer to a container image, e.g. registry.io/org/image:tag ELEMENTAL_NO_VERIFY: Turns off or on mtree verification. ELEMENTAL_RECOVERY_IMAGE: Allows to specify a different image for the recovery partition. Similarly to UPGRADE_IMAGE needs to be either an image reference or a package.  Changing the default release channel Release channels are standard luet repositories. To change the default release channel, create a /etc/luet/luet.yaml configuration file pointing to a valid luet repository:\n# For a full reference, see:# https://luet-lab.github.io/docs/docs/getting-started/#configurationlogging:color:falseenable_emoji:falsegeneral:debug:falsespinner_charset:9repositories:- name:\"sampleos\"description:\"sampleOS\"type:\"docker\"enable:truecached:truepriority:1verify:falseurls:- \"quay.io/costoolkit/releases-green\"","categories":"","description":"Customizing the default upgrade channel\n","excerpt":"Customizing the default upgrade channel\n","ref":"/docs/customizing/upgrades/","tags":"","title":"Upgrades"},{"body":"Requirements:\n qemu-img utility elemental binary elemental runtime dependencies  The suggested approach is based on using the Elemental installer (elemental install command) to run the installation from a Linux to a loop device. The loop device can be a raw image created with qemu-img create that can easily be converted to other formats after the installation by using qemu-img convert.\nGet Elemental Elemental binary can be downloaded from the github releases page.\nThe golang binary can be used as is, however take into account that some linux utilities are expected to be present in the host. More specific elemental expects to find common linux utilities to operate over block devices: rsync, parted, blkid, lsblk, udevadm, resize2fs, tune2fs, mkfs.ext2, etc.\nPrepare the loop device Preparing the a loop device for the installation is simple and straight forward.\n# Create a raw image of 32G \u003e qemu-img create -f raw diks.img 32G # Set the disk image as a loop device \u003e sudo losetup -f --show disk.img \u003cdevice\u003e Run elemental installation Execute the elemental installation as described in TODO:\n\u003e sudo elemental install --force-efi --docker-image \u003cimage\u003e \u003cdevice\u003e Where \u003cimage\u003e is the cOS derivative container image we want to use for the disk creation and \u003cdevice\u003e is the loop device previously created with losetup (e.g. /dev/loop0).\nConvert the RAW image to desired format Once the installation is done just unsetting the loop device and converting the image to the desired format is missing:\n# Unset the loop device \u003e sudo losetup -d \u003cdevice\u003e # Convert the RAW image to qcow2 \u003e qemu-img convert -f raw -O qcow2 disk.img disk.qcow2 QEMU supports a wide range of formats including common ones such as vdi, vmdk or vhdx.\nThe result can be easily tested on QEMU with:\n\u003e qemu -m 4096 -hda disk.qcow2 -bios /usr/share/qemu/ovmf-x86_64.bin Note the firmware image path varies depending on the host distro, the path provided in this example is based on openSUSE Leap.\n","categories":"","description":"This section documents the procedure to build disk images using elemental\n","excerpt":"This section documents the procedure to build disk images using …","ref":"/docs/creating-derivatives/build_disk/","tags":"","title":"Build disk images with Elemental"},{"body":"This section describes the runtime layout of a derivative (or a cOS Vanilla image) once booted in a system.\nThe cOS toolkit performs during installation a common setup which is equivalent across all derivatives.\nThis mechanism ensures that a layout:\n it’s simple and human friendly allows to switch easily derivatives allows to perform recovery tasks is resilient to upgrade failures  Layout The basic setup consists of:\n an A/B partitioning style. We have an ‘active’ and a ‘passive’ system too boot from in case of failures a Recovery system which allows to perform emergency tasks in case of failure of the ‘A/B’ partitions a Fallback mechanism that boots the partitions in this sequence: “A -\u003e B -\u003e Recovery” in case of booting failures  The upgrade happens in a transition image and take places only after all the necessary steps are completed. An upgrade of the ‘A/B’ partitions can be done by booting into them and running elemental upgrade. This will create a new pristine image that will be selected as active for the next reboot, the old one will be flagged as passive. If we are performing the same from the passive system, only the active is subject to changes.\nSimilarly, a recovery system can be upgraded as well by running elemental upgrade --recovery. This will upgrade the recovery system instead of the active/passive. Note both commands needs to be run inside the active or passive system.\nPartitions The default partitioning is created during installation and is expected to be present in a booted cOS system:\n a COS_STATE partition that will contain our active, passive and recovery images. The images are located under the /cOS directory a COS_PERSISTENT partition which contains the persistent user data. This directory is mounted over /usr/local during runtime a COS_OEM partition which contains the cloud-init oem files, which is mounted over /oem during runtime a COS_RECOVERY partition which contains the recovery system image  The COS_STATE partitions contains the active, passive . While the active and passive are .img files which are loopback mounted, the recovery system is in COS_RECOVERY and can also be a squashfs file (provided in /cOS/recovery.squashfs). This ensures the immutability aspect and ease out building derivative in constrained environments (e.g. when we have restricted permissions and we can’t mount).\nFor more information about the immutability aspect of cOS, see Immutable rootfs\n","categories":"","description":"Runtime layout of a booted cOS derivative\n","excerpt":"Runtime layout of a booted cOS derivative\n","ref":"/docs/reference/layout/","tags":"","title":"Runtime layout"},{"body":"Requirements:\n Packer AWS access keys with the appropriate roles and permissions A Vanilla AMI Packer templates  The suggested approach is based on using Packer templates to customize the deployment and automate the upload and publish to AWS of cOS derivatives or cOS itself. For all the details and possibilties of Packer check the official documentation.\nRun the build with Packer Publishing an AMI image in AWS based on top of the latest cOS Vanilla image is fairly simple. In fact, it is only needed to set the AWS credentials and run a packer build process to trigger the deployment and register the resulting snapshot as an AMI. In such case the lastest cOS image will be deployed and configured with pure defaults. Consider:\n# From the root of a cOS-toolkit repository checkout \u003e export AWS_ACCESS_KEY_ID=\u003cyour_aws_access_key\u003e \u003e export AWS_SECRET_ACCESS_KEY=\u003cyour_aws_secret_access_key\u003e \u003e export AWS_DEFAULT_REGION=\u003cyour_aws_default_region\u003e \u003e cd packer \u003e packer build -only amazon-ebs.cos . AWS keys can be passed as environment variables as it is above or packer picks them from aws-cli configuration files (~/.aws) if any. Alternatively, one can define them in the variables file.\nThe -only amazon-ebs.cos flag is just to tell packer which of the sources to make use for the build. Note the packer/images.json.pkr.hcl file defines few other sources such as qemu and virtualbox.\nCustomize the build with a variables file The packer template can be customized with the variables defined in packer/variables.pkr.hcl. These are the variables that can be set on run time using the -var key=value or -var-file=path flags. The variable file can be a json file including desired varibles. Consider the following example:\n# From the packer folder of the cOS-toolkit repository checkout \u003e cat \u003c\u003c EOF \u003e test.json { \"aws_cos_deploy_args\": \"cos-deploy\", \"aws_launch_volume_size\": 16, \"name\": \"MyTest\" } EOF \u003e packer build -only amazon-ebs.cos -var-file=test.json . The above example runs the AMI Vanilla image on a 16GiB disk and calls the command cos-deploy to deploy the main OS, once deployed an snapshot is created and an AMI out this snapshot is registered in EC2. The created AMI artifact will be called MyTest, the name has no impact in the underlaying OS.\nAvailable variables for customization All the customizable variables are listed in packer/variables.pkr.hcl, variables with the aws_ prefix are the ones related to the AWS Packer template. These are some of the relevant ones:\n  aws_cos_deploy_args: This the command that will be executed once the Vanilla image booted. In this stage it is expected that user sets a command to install the desired cOS or derivative image. By default it is set to cos-deploy which will deploy the latest cOS image in cOS repositories. To deploy custom derivatives something like cos-deploy --docker-image \u003cmy-derivative-img-ref\u003e should be sufficient.\n  aws_launch_volume_size: This sets the disk size of the VM that Packer launches for the build. During Vanilla image first boot the system will expand to the disk geometry. The layout is configurable with the user-data.\n  aws_user_data_file: This sets the user-data file that will be used for the aws instance during the build process. It defaults to aws/setup-disk.yaml and the defauklt file basically includes the disk expansion configuration. It adds a COS_STATE partition that should be big enough to store about three times the size of the image to deploy. Then it also creates a COS_PERSISTENT partition with all the rest of the available space in disk.\n  aws_source_ami_filter_name: This a filter to choose the AMI image for the build process. It defaults to *cOS*Vanilla* pattern to pick the latest cOS Vanilla image available.\n  aws_temporary_security_group_source_cidr: A IPv4 CIDR to be authorized access to the instance, when packer is creating a temporary security group. Defaults to “0.0.0.0/0”.\n  ","categories":"","description":"This section documents the procedure to deploy cOS (or derivatives) images in AWS public cloud provider by using the cOS Vanilla image.\n","excerpt":"This section documents the procedure to deploy cOS (or derivatives) …","ref":"/docs/creating-derivatives/packer/build_ami/","tags":"","title":"Build AMI machines for AWS"},{"body":"Requirements:\n Packer Azure access keys with the appropriate roles and permissions A Vanilla image Packer templates  The suggested approach is based on using Packer templates to customize the deployment and automate the upload and publish to Azure of cOS derivatives or cOS itself. For all the details and possibilities of Packer check the official documentation.\nRun the build with Packer Publishing an image in Azure based on top of the latest cOS Vanilla image is fairly simple. In fact, it is only needed to set the Azure credentials and run a packer build process to trigger the deployment and register the resulting snapshot as an image. In such case the latest cOS image will be deployed and configured with pure defaults. Consider:\n# From the root of a cOS-toolkit repository checkout \u003e export AZURE_CLIENT_ID=\u003cyour_azure_client_id\u003e \u003e export AZURE_TENANT_ID=\u003cyour_azure_tenant_id\u003e \u003e export AZURE_CLIENT_SECRET=\u003cyour_azure_client_secret\u003e \u003e export AZURE_SUBSCRIPTION_ID=\u003cyour_azure_subscription_id\u003e \u003e cd packer \u003e packer build -only azure-arm.cos . The -only azure-arm.cos flag is just to tell packer which of the sources to make use for the build. Note the packer/images.json.pkr.hcl file defines few other sources such as qemu and virtualbox.\nCustomize the build with a variables file The packer template can be customized with the variables defined in packer/variables.pkr.hcl. These are the variables that can be set on run time using the -var key=value or -var-file=path flags. The variable file can be a json file including desired variables. Consider the following example:\n# From the packer folder of the cOS-toolkit repository checkout \u003e cat \u003c\u003c EOF \u003e test.json { \"azure_location\": \"westeurope\", \"azure_os_disk_size_gb\": 16, \"azure_vm_size\": \"Standard_B2s\" } EOF \u003e packer build -only azure-arm.cos -var-file=test.json . The above example runs the Vanilla image on a 16GiB disk and calls the command cos-deploy to deploy the main OS and once deployed an image is created from the running instance.\nAvailable variables for customization All the customizable variables are listed in packer/variables.pkr.hcl, variables with the azure_ prefix are the ones related to the Azure Packer template. These are some of the relevant ones:\n  azure_cos_deploy_args: This the command that will be executed once the Vanilla image booted. In this stage it is expected that user sets a command to install the desired cOS or derivative image. By default it is set to cos-deploy which will deploy the latest cOS image in cOS repositories. To deploy custom derivatives something like cos-deploy --docker-image \u003cmy-derivative-img-ref\u003e should be sufficient.\n  azure_custom_managed_image_name: Name of a custom managed image to use for your base image.\n  azure_custom_managed_image_resource_group_name: Name of a custom managed image’s resource group to use for your base image.\n  azure_os_disk_size_gb: This sets the disk size of the VM that Packer launches for the build. During Vanilla image first boot the system will expand to the disk geometry. The layout is configurable with the user-data.\n  azure_vm_size: Sets the size of the instance being launched. Defaults to Standard_B2s\n  azure_user_data_file: This sets the user-data file that will be used for the azure instance during the build process. It defaults to user-data/azure.yaml and the default file basically includes the disk expansion configuration. It adds a COS_STATE partition that should be big enough to store about three times the size of the image to deploy. Then it also creates a COS_PERSISTENT partition with all the rest of the available space in disk.\n  The current version of packer (1.7.4) doesn’t have any support for user-data so currently is not possible to automate the deployment with packer correctly. Have a look at packer changelog to be informed when user-data is supported.\n ","categories":"","description":"This section documents the procedure to deploy cOS (or derivatives) images in Azure public cloud provider by using the cOS Vanilla image.\n","excerpt":"This section documents the procedure to deploy cOS (or derivatives) …","ref":"/docs/creating-derivatives/packer/build_azure/","tags":"","title":"Build Azure images"},{"body":"Requirements:\n Packer Google Compute Packer plugin Google Cloud SDK A Vanilla AMI Packer templates  The suggested approach is based on using Packer templates to customize the deployment and automate the upload and publish to GCP of cOS derivatives or cOS itself. For all the details and possibilties of Packer check the official documentation.\nThere are no cOS Vanilla images publicly available in GCP, however they can be easily build or downloaded and published to your working GCP project. See Build Raw Images and Importing a Google Cloud image manually to see how to upload a Vanilla image in your project.\nRun the build with Packer Publishing an image in GCP based on top of the latest cOS Vanilla image is fairly simple. In fact, it is only needed to set the User Application Default Credentials for GCP and the GCP project ID and then run a packer build process to trigger the deployment and register the resulting snapshot as an image. In such case the lastest cOS image will be deployed and configured with pure defaults. Consider:\n# From the root of a cOS-toolkit repository checkout \u003e export GCP_PROJECT_ID=\u003cyour_gcp_project_id\u003e \u003e cd packer \u003e packer build -only gcp.cos . Packer authenticates automatically if the User Application Default Credentials are properly set in the host.\nThe -only gcp.cos flag is just to tell packer which of the sources to make use for the build. Note the packer/images.json.pkr.hcl file defines few other sources such as qemu, virtualbox and amazon-ebs.\nCustomize the build with a variables file The packer template can be customized with the variables defined in packer/variables.pkr.hcl. These are the variables that can be set on run time using the -var key=value or -var-file=path flags. The variable file can be a json file including desired varibles. Consider the following example:\n# From the packer folder of the cOS-toolkit repository checkout \u003e cat \u003c\u003c EOF \u003e test.json { \"gcp_project_id\": \"\u003cyour_gcp_project\u003e\" \"gcp_cos_deploy_args\": \"cos-deploy --docker-image \u003cmy-custom-image\u003e\", \"gcp_disk_size\": 20, \"name\": \"MyTest\" } EOF \u003e packer build -only gcp.cos -var-file=test.json . The above example runs the cOS Vanilla image on a 20GB disk and calls the command cos-deploy to deploy the main OS, once deployed an snapshot is created and published as an image in Google Compute Engine. The created artifact will be called MyTest, the name has no impact in the underlaying OS.\nAvailable variables for customization All the customizable variables are listed in packer/variables.pkr.hcl, variables with the aws_ prefix are the ones related to the AWS Packer template. These are some of the relevant ones:\n  gcp_project_id: The project ID that will be used to launch instances and store images.\n  gcp_cos_deploy_args: This the command that will be executed once the Vanilla image booted. In this stage it is expected that user sets a command to install the desired cOS or derivative image. By default it is set to cos-deploy which will deploy the latest cOS image in cOS repositories. To deploy custom derivatives something like cos-deploy --docker-image \u003cmy-derivative-img-ref\u003e should be sufficient.\n  gcp_disk_size: This sets the disk size of the VM that Packer launches for the build. During Vanilla image first boot the system will expand to the disk geometry. The layout is configurable with the user-data.\n  gcp_user_data_file: This sets the user-data file that will be used for the aws instance during the build process. It defaults to aws/setup-disk.yaml and the defauklt file basically includes the disk expansion configuration. It adds a COS_STATE partition that should be big enough to store about three times the size of the image to deploy. Then it also creates a COS_PERSISTENT partition with all the rest of the available space in disk.\n  gcp_source_image_family: This the family to choose the image for the build process. It defaults to cos-vanilla to pick the latest cOS Vanilla image available. Note Packer tries to find the image family first in the given working project (gcp_project_id).\n  ","categories":"","description":"This section documents the procedure to deploy cOS (or derivatives) images in Google Compute Platform by using the cOS Vanilla image.\n","excerpt":"This section documents the procedure to deploy cOS (or derivatives) …","ref":"/docs/creating-derivatives/packer/build_gcp/","tags":"","title":"Build Images for Google Compute Platform"},{"body":"In order to build an iso at the moment of writing, we first rely on luet-makeiso. It accepts a YAML file denoting the packages to bundle in an ISO and a list of luet repositories where to download the packages from.\nTo build an iso, just run:\ndocker run -v $PWD:/cOS -v /var/run:/var/run --entrypoint /usr/bin/luet-makeiso -ti --rm quay.io/costoolkit/toolchain ./iso.yaml --image $IMAGE Where iso.yaml is the iso specification file, and --image $IMAGE is the container image you want to build the ISO for, you might want to check on how to build bootable images.\nAn example of a yaml file using the cos-toolkit opensuse repositories and syslinux:\npackages:rootfs:- system/cosuefi:- live/systemd-boot- live/bootisoimage:- live/syslinux- live/bootinitramfs:kernel_file:\"vmlinuz\"rootfs_file:\"initrd\"image_prefix:\"cOS-0.\"image_date:truelabel:\"COS_LIVE\"An example using GRUB instead:\npackages:rootfs:- ...uefi:- live/grub2-efi-imageisoimage:- live/grub2- live/grub2-efi-image- recovery/cos-imgboot_file:\"boot/x86_64/loader/eltorito.img\"boot_catalog:\"boot/x86_64/boot.catalog\"isohybrid_mbr:\"boot/x86_64/loader/boot_hybrid.img\"initramfs:kernel_file:\"vmlinuz\"rootfs_file:\"initrd\"image_prefix:\"cOS-0.\"image_date:truelabel:\"COS_LIVE\"What’s next?  Check out on how to build a QCOW, Virtualbox or Vagrant image from the ISO we have just created  Syntax Below you can find a full reference about the yaml file format.\npackages:# (optional) Packages to be installed in the rootfsrootfs:- ..# Packages to be installed in the uefi imageuefi:- live/systemd-boot- live/boot# Packages to be installed in the isoimageisoimage:- live/syslinux# Specify initramfs/kernel and avoid generation on-the-fly# files must be present on /boot folder in the rootfsinitramfs:kernel_file:\"vmlinuz\"rootfs_file:\"initrd\"overlay:rootfs:\"/path/to/additional/files\"uefi:\"/path/to/additional/uefi/files\"isoimage:\"/path/to/additional/efi/files\"# Specify a container remote image to pull and use for the rootfs in place of packages (optional)rootfs_image:\"ubuntu:latest\"# Image prefix. If Image date is disabled is used as the full title.image_prefix:\"MYOS-0.\"image_date:true# Luet repositories (https://luet-lab.github.io/docs/docs/getting-started/#configuration-in-etcluetreposconfd) to use.luet:repositories:- name:cOSenable:trueurls:- quay.io/costoolkit/releases-greentype:dockerFlags:\n local: Path to local luet repository to use to install packages from image: Optional image to use as rootfs  Configuration reference rootfs_image A container image reference (e.g. quay.io/.../...:latest) that will be used as a rootfs for the ISO.\npackages.rootfs A list of luet packages to install in the rootfs. The rootfs will be squashed to a rootfs.squashfs file\npackages.uefi A list of luet packages to be present in the efi ISO sector.\npackages.isoimage A list of luet packages to be present in the ISO image.\nrepository.packages A list of package repository (e.g. repository/mocaccino-extra) to be installed before luet install commands Requirements\ninitramfs.kernel_file The kernel file under /boot/ that is your running kernel. e.g. vmlinuz or bzImage\ninitramfs.rootfs_file The initrd file under /boot/ that has all the utils for the initramfs\nimage_prefix ISO image prefix to use\nimage_date Boolean indicating if the output image name has to contain the date\nimage_name A string representing the ISO final image name\narch A string representing the arch. Defaults to x86_64.\nluet.config Path to the luet config to use to install the packages from\noverlay.isoimage overlay:isoimage:\"dir\"Path to a folder which content will be copied over the ISO (before generating the ISO file).\noverlay.uefi overlay:uefi:\"dir\"Path to a folder which content will be copied over the UEFI partition (before generating the image).\noverlay.rootfs overlay:rootfs:\"dir\"Path to a folder which content will be copied over the rootfs (before generating squashfs).\nCustomize bootloader with GRUB Beside syslinux, the ISO boot menu can also be built with GRUB.\nBoot menu and other bootloader parameters can then be easily customized by using the overlay parameters within the ISO config yaml manifest.\nAssuming the ISO being built includes:\npackages:rootfs:- ...uefi:- live/grub2-efi-imageisoimage:- live/grub2- live/grub2-efi-image- recovery/cos-img# The following are required in order to build an ISO with GRUB# as bootloaderboot_file:\"boot/x86_64/loader/eltorito.img\"boot_catalog:\"boot/x86_64/boot.catalog\"isohybrid_mbr:\"boot/x86_64/loader/boot_hybrid.img\"We can customize either the isoimage packages (in the referrence image live/grub2 package includes bootloader configuration) or make use of the overlay concept to include or overwrite addition files for isoimage section.\nConsider the following example:\npackages:rootfs:- system/cosuefi:- live/grub2-efi-imageisoimage:- live/grub2- live/grub2-efi-image- recovery/cos-imgoverlay:isoimage:overlay/isoWith the above the ISO will also include the files under overlay/iso path. To customize the boot menu parameters consider copy and modify relevant files from live/grub2 package. In this example the overlay folder files list could be:\n# isoimage files for grub2 boot boot/grub2/grub.cfg Being boot/grub2/grub.cfg a custom grub2 configuration including custom boot menu entries. Consider the following grub.cfg example:\nsearch --file --set=root /boot/kernel.xz set default=0 set timeout=10 set timeout_style=menu set linux=linux set initrd=initrd if [ \"${grub_cpu}\" = \"x86_64\" -o \"${grub_cpu}\" = \"i386\" ];then if [ \"${grub_platform}\" = \"efi\" ]; then set linux=linuxefi set initrd=initrdefi fi fi set font=($root)/boot/x86_64/loader/grub2/fonts/unicode.pf2 if [ -f ${font} ];then loadfont ${font} fi menuentry \"Custom grub2 menu entry\" --class os --unrestricted { echo Loading kernel... $linux ($root)/boot/kernel.xz cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable echo Loading initrd... $initrd ($root)/boot/rootfs.xz } Separate recovery To make an ISO with a separate recovery image as squashfs, you can either use the default from cOS, by adding it in the iso yaml file:\npackages:rootfs:..uefi:..isoimage:...- recovery/cos-imgThe installer will detect the squashfs file in the iso, and will use it when installing the system. You can customize the recovery image as well by providing your own: see the recovery/cos-img package definition as a reference.\n","categories":"","description":"Build ISOs from bootable images\n","excerpt":"Build ISOs from bootable images\n","ref":"/docs/creating-derivatives/build_iso/","tags":"","title":"Build ISOs"},{"body":"Requirements:\n Packer Either qemu or VirtualBox functioning in the build host a cOS or a custom ISO Packer templates  The suggested approach is based on using Packer templates to customize the deployment and automate creation of QCOW, Virtualbox and Vagrant images of cOS derivatives or cOS itself. For all the details and possibilties of Packer check the official documentation.\nRun the build with Packer To build QCOW and VirtualBox images an ISO file is required. You can either Download a cOS ISO or build your own from a container image.\nQCOW2 Consider:\n# From the root of a cOS-toolkit repository checkout \u003e cd packer \u003e packer build -var \"iso=/path/to/image.iso\" -only qemu.cos . The process should end up by building a .box file (vagrant image) and a .tar.gz file (containing the raw disk) in the same folder:\n... ==\u003e qemu.cos (compress): Using pgzip compression with 8 cores for cOS_green_dev_amd64.tar.gz ==\u003e qemu.cos (compress): Tarring cOS_green_dev_amd64.tar.gz with pgzip ==\u003e qemu.cos (compress): Archive cOS_green_dev_amd64.tar.gz completed Build 'qemu.cos' finished after 4 minutes 34 seconds. ==\u003e Wait completed after 4 minutes 34 seconds ==\u003e Builds finished. The artifacts of successful builds are: --\u003e qemu.cos: 'libvirt' provider box: cOS_green_dev_amd64.box --\u003e qemu.cos: compressed artifacts in: cOS_green_dev_amd64.tar.gz \u003e ubuntu@jumpbox:~/cOS-toolkit/packer$ ls -liah total 2.3G 516552 drwxrwxr-x 4 ubuntu ubuntu 4.0K Jul 30 07:54 . 516207 drwxrwxr-x 14 ubuntu ubuntu 4.0K Jul 30 07:41 .. 516362 -rw-r--r-- 1 root root 1.2G Jul 30 07:54 cOS_green_dev_amd64.box 516385 -rw-r--r-- 1 root root 1.2G Jul 30 07:54 cOS_green_dev_amd64.tar.gz 516553 -rw-rw-r-- 1 ubuntu ubuntu 389 Jun 28 08:07 config.yaml 516339 -rw-rw-r-- 1 ubuntu ubuntu 6.5K Jul 30 07:41 images.json.pkr.hcl 1818463 drwxrwxr-x 3 ubuntu ubuntu 4.0K Jul 30 07:49 packer_cache 1818400 drwxrwxr-x 2 ubuntu ubuntu 4.0K Jul 30 07:41 user-data 516555 -rw-rw-r-- 1 ubuntu ubuntu 606 Jun 28 08:07 vagrant.yaml 516340 -rw-rw-r-- 1 ubuntu ubuntu 6.1K Jul 30 07:41 variables.pkr.hcl ubuntu@jumpbox:~/cOS-toolkit/packer$ The -only qemu.cos flag is just to tell packer which of the sources to make use for the build. Note the packer/images.json.pkr.hcl file defines few other sources such as amazon-ebs and virtualbox-iso.\nVirtualbox Similarly, to build OVA images we run:\n# From the root of a cOS-toolkit repository checkout \u003e cd packer \u003e packer build -var \"iso=/path/to/image.iso\" -only virtualbox-iso.cos . Vagrant To build vagrant images, we enable the vagrant feature, which allows the vagrant user to login afterwards:\n# From the root of a cOS-toolkit repository checkout \u003e cd packer \u003e packer build -var \"iso=/path/to/image.iso\" -var \"feature=vagrant\" -only virtualbox-iso.cos . Customize the build with a variables file The packer template can be customized with the variables defined in packer/variables.pkr.hcl. These are the variables that can be set on run time using the -var key=value or -var-file=path flags. The variable file can be a json file including desired varibles. Consider the following example:\n# From the packer folder of the cOS-toolkit repository checkout \u003e cat \u003c\u003c EOF \u003e test.json { \"root_username\": \"root\", \"root_password\": \"cos\" } EOF \u003e packer build -only qemu.cos -var \"iso=/path/to/image.iso\" -var-file=test.json . The above example runs the build by logging with a different username/password to run the installation.\nDefault cloud-init In the packer folder there is present a config.yaml file which can be used to customize the image. The file is in cloud-init style and will be automatically installed by default when running elemental install.\nAvailable variables for customization All the customizable variables are listed in packer/variables.pkr.hcl, these are some of the relevant ones:\n  build,flavor,arch: This to personalize the artifact output name. The artifact output format is \"cOS_${var.flavor}_${var.build}_${var.arch}.tar.gz\"\n  disk_size: This sets the disk size to be used for the image. It is 50000 by default, and expressed in MB.\n  memory: RAM to allocate to the VM in MB.\n  cpu: Number of processors of the VM\n  accelerator: Accelerator type, see the official docs\n  root_username: Username used to login via ssh, it needs root permissions to be able to run the installation process and call elemental install\n  root_password: Password for the user specified in root_username\n  feature: Enable/Disables specific cOS features.\n  ","categories":"","description":"This section documents the procedure to build a custom QCOW, VirtualBox and a Vagrant images with the cOS packer templates\n","excerpt":"This section documents the procedure to build a custom QCOW, …","ref":"/docs/creating-derivatives/packer/build_images/","tags":"","title":"Build QCOW, VirtualBox and Vagrant images"},{"body":"Requirements:\n cOS-toolkit source  The suggested approach high level view is building cOS packages and generating a RAW image from them. That would allow us to transform that RAW image in a valid Azure/Google/Amazon Cloud blob that can be transformed into a VM image ready to be launched.\nThis generates a vanilla image that boots into recovery mode and can be used to deploy whatever image you want to the VM. Then you can snapshot that VM into a VM image ready to deploy with the default cOS system or your derivative.\nThe RAW image can then be used into packer templates to generate custom Images, or used as-is with a userdata to deploy a container image of choice with an input user-data.\nBuilding the packages We need to have the packages built locally in order to generate a proper RAW image. Just run:\nsudo make build All the artifacts will be generated under the build directory.\nBuilding the RAW image The RAW image is just a RAW disk image that contains the recovery, so once launched is ready to be used for installing whatever cOS or derivative that you want into the VM disks. This allows us to have a barebones base image that can be used for provisioning whatever cOS you want.\nBuilding the RAW image is as simple as running:\nsudo make raw_disk This will output a disk.raw file in the current directory which can be run with qemu, see booting.\nAWS No other steps are required, the raw disk can be already booted as an AMI image, see booting.\nAzure Requirements:\n Azure Cloud access keys with the appropriate roles and permissions azure-cli  Transform the RAW image into a compatible Azure Cloud blob Currently importing images from storage blobs on Azure Cloud have a few requirements:\n only fixed VHD format is supported VHD disk must have a virtual size aligned to 1 MB LVM is not supported no swap partition  We provide a make target that will do this for you:\nsudo make azure_disk This will create a disk.vhd which is our final artifact\nUploading to Azure Cloud storage and importing it as an image The last step is to upload the blob to Azure Cloud storage and import that blob as a valid image.\nWith azure-cli installed and its credentials configured, you upload the blob with:\naz storage copy --source \u003ccos-azure-image\u003e --destination https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003cdestination-cos-azure-image\u003e And import it as an image with:\naz image create --resource-group \u003cresource-group\u003e --source https://\u003caccount\u003e.blob.core.windows.net/\u003ccontainer\u003e/\u003ccos-azure-image\u003e --os-type linux --hyper-v-generation v2 --name \u003cimage-name\u003e Where cos-azure-image is the blob we just uploaded, basically you can use the same value as you set in --destionation for the upload\nNote that we used --os-type linux --hyper-v-generation v2 as flags. This indicates that the image will be booted with UEFI and its required. Otherwise, launching the image will try to boot it in legacy mode, and it will fail.\nOnce this is over you will have you cOS (or derivative) vanilla image ready for consumption. You can see your new image by running:\naz image show --resource-group \u003cresource-group\u003e --name \u003cimage-name\u003e Google cloud Requirements:\n Google Cloud access keys with the appropriate roles and permissions gsutil and gcloud tools  Transform the RAW image into a compatible Google Cloud blob Currently importing images from storage blobs on Google Cloud have a few requirements:\n blobs have to be tar.gzipped with the flag --format=oldgnu the disk.raw has to be rounded up to the next Gb ( so no 2.1gb images or 2.4, they need to be an exact 3Gb or 2Gb) image inside the tar.gzip blob needs to be called disk.raw  We provide a make target that will do this for you:\nsudo make gce_disk This will create a disk.raw.tar.gz which is our final artifact\nUploading to Google Cloud storage and importing it as an image The last step is to upload the blob to Google Cloud storage and import that blob as a valid image.\nWith gsutil and gcloud tools installed and their credentials configured, you upload the blob with:\ngsutil cp disk.raw.tar.gz gs://YOURBUCKET/ Where YOURBUCKET is the destination bucket you want your file to end up in.\nAnd import it as an image with:\ngcloud compute images create IMAGENAME --source-uri=SOURCEURI --guest-os-features=UEFI_COMPATIBLE Where:\n IMAGENAME: The name for the final image SOURCEURI: The full Google Cloud Storage URI where the disk image is stored. This file must be a gzip-compressed tarball whose name ends in .tar.gz. This is the full path to the blob we just uploaded.  Note that we used --guest-os-features=UEFI_COMPATIBLE as a flag. This indicates that the image will be booted with UEFI and its required. Otherwise, launching the image will try to boot it in legacy mode and it will fail.\nOnce this is over you will have you cOS (or derivative) vanilla image ready for consumption. You can see your new image by running:\ngcloud compute images describe IMAGENAME ","categories":"","description":"This section documents the procedure to build cOS raw images which are used to boot into Cloud providers.\n","excerpt":"This section documents the procedure to build cOS raw images which are …","ref":"/docs/development/build_raw_images/","tags":"","title":"Build Raw images"},{"body":"A derivative is a standard container image which can be booted by cOS.\nWe can identify a build phase where we build the derivative, and a “runtime phase” where we consume it.\nThe image is described by a Dockerfile, composed of a base OS of choice (e.g. openSUSE, Ubuntu, etc. ) and the cOS toolkit itself in order to be consumed by cOS and allow to be upgraded from by other derivatives.\ncOS-toolkit then converts the OCI artifact into a bootable medium (ISO, packer, ova, etc) and the image itself then can be used to bootstrap other derivatives, which can in turn upgrade to any derivative built with cOS.\nA derivative can also be later re-used again as input as base-image for downstream derivatives.\nAll the documentation below imply that the container image generated will be the booting one, there are however several configuration entrypoint that you should keep in mind while building the image which are general across all the implementation:\n Custom persistent runtime configuration has to be provided in /system/oem for derivatives, see also the documentation section. Everything under /system/oem will be loaded during the various stages (boot, network, initramfs). You can check here for the cOS defaults. See 00_rootfs.yaml to customize the booting layout. /etc/cos/bootargs.cfg contains the booting options required to boot the image with GRUB, see grub customization /etc/cos-upgrade-image contains the default upgrade configuration for recovery and the booting system image, see customizing upgrades  Derivatives inherits cOS defaults, which you can override during the build process, however there are some defaults which are relevant and listed below:\n","categories":"","description":"Documents various methods for creating cOS derivatives\n","excerpt":"Documents various methods for creating cOS derivatives\n","ref":"/docs/creating-derivatives/","tags":"","title":"Creating derivatives"},{"body":"COS is set to deploy a persistent grub.cfg into the COS_RECOVERY partition during the system installation or image creation. COS grub configuration includes three menu entries: first for the main OS system, second for the fallback OS system and a third for the recovery OS.\nFor example the main OS system menu entry could be something like:\nmenuentry \"cOS\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd initrd (loop0)$initramfs }  Kernel parameters The kernel parameters are not part of the persistent grub.cfg file stored in COS_RECOVERY partition. Kernel parameters are sourced from the loop device of the OS image to boot. This is mainly to keep kernel parameters consistent across different potential OS images or system upgrades.  Specifying default custom boot options cOS images and its derivatives, are expected to include a /etc/cos/bootargs.cfg file which provides the definition of the following variables:\n $kernel: Path of the kernel binary $kernelcmd: Kernel parameters $initramfs: Path of the initrd binary  This is the mechanism any cOS image or cOS derivative has to communicate its boot parameters (kernel, kernel params and initrd file) to GRUB2.\nFor example, the default cOS bootarg.cfg file is:\nset kernel=/boot/vmlinuz if [ -n \"$recoverylabel\" ]; then # Boot arguments when the image is used as recovery set kernelcmd=\"console=tty1 root=live:CDLABEL=$recoverylabel rd.live.dir=/ rd.live.squashimg=$img panic=5\" else # Boot arguments when the image is used as active/passive set kernelcmd=\"console=tty1 root=LABEL=$label iso-scan/filename=$img panic=5 security=selinux rd.cos.oemlabel=COS_OEM selinux=1\" fi set initramfs=/boot/initrd You can tweak that file to suit your needs if you need to specify persistent boot arguments.\nNote rd.cos.oemlabel=COS_OEM is required inside the bootargs in order to access to the /oem mount within the rootfs stage. COS_OEM is the default label for the oem partition.  Grub environment variables cOS (since v0.5.8) makes use of the GRUB2 environment block which can used to define persistent GRUB2 variables across reboots.\nUse grub2-editenv command line utility to define the desired values.\n   Variable Description     next_entry Set the next reboot entry   saved_entry Set the default boot entry   default_menu_entry Set the name entries on the GRUB menu   extra_active_cmdline Set additional boot commands when booting into active   extra_passive_cmdline Set additional boot commands when booting into passive   extra_recovery_cmdline Set additional boot commands when booting into recovery   extra_cmdline Set additional boot commands for all entries   default_fallback Sets default fallback logic    For instance use the following command to reboot to recovery system only once:\n\u003e grub2-editenv /oem/grubenv set next_entry=recovery  Note The examples below make of the COS_OEM device, however it could use any device detected by GRUB2 that includes the file /grubenv. First match wins.  Default boot entry The default grub configuration loads the /grubenv of any available device and evaluates on next_entry variable and saved_entry variable. By default none is set.\nThe default boot entry is set to the value of saved_entry, in case the variable is not set grub just defaults to the first menu entry.\nnext_entry variable can be used to overwrite the default boot entry for a single boot. If next_entry variable is set this is only being used once, GRUB2 will unset it after reading it for the first time. This is helpful to define the menu entry to reboot to without having to make any permanent config change.\nUse grub2-editenv command line utility to define desired values.\nFor instance use the following command to reboot to recovery system only once:\n\u003e grub2-editenv /oem/grubenv set next_entry=recovery Or to set the default entry to fallback system:\n\u003e grub2-editenv /oem/grubenv set saved_entry=fallback Boot menu By default cOS and derivatives shows the default boot menu entry while booting (cOS).\nThe grub menu entry is generated during installation and can be configured by setting GRUB_ENTRY_NAME in the cOS configuration file inside the derivative, or either via cloud-init before installation.\nFor example, specifying in /etc/cos/config:\nGRUB_ENTRY_NAME=myOS will automatically set the GRUB menu entries for active, passive and recovery to the specified value.\nThe grub menu boot entry can also be set with grub2-editenv:\n\u003e grub2-editenv /oem/grubenv set default_menu_entry=fooOS  Additional menu entries Since\n system/grub2-config \n = 0.0.14 it is possible to add multiple custom menu entries to GRUB by creating a /grubmenu config file in one of the available partitions detected by GRUB2 during boot. The file will be loaded from the first partition found by GRUB that have the grubmenu file. First match wins. The grubmenu file will be sourced at the end of the boot process, and can contain several menuentry blocks.\n  Persistent boot option flags It is possible to define persistent boot flag for each menu entry also via grub2-editenv:\n extra_active_cmdline: extra bootflags to be applied only on active boot extra_passive_cmdline: extra bootflags to be applied only on passive boot extra_recovery_cmdline: extra bootflags to be applied only on recovery extra_cmdline: will be applied to each boot entry  Customizing fallback logic By default cOS boots into active, and if there are failures will boot into the passive, and finally if keeps failing, will boot into recovery.\nIt is possible to override the default fallback logic by setting default_fallback as grub environment, consider for example:\n\u003e grub2-editenv /oem/grubenv set default_fallback=\"2 0 1\" Will set the default fallback to “2 0 1” instead of the default “0 1 2”.\n","categories":"","description":"GRUB 2 Configuration\n","excerpt":"GRUB 2 Configuration\n","ref":"/docs/customizing/configure_grub/","tags":"","title":"GRUB"},{"body":"The immutable rootfs concept in cOS is provided by a dracut module which is basically the contents of the immutable-rootfs package provided as part of the cOS repository tree.\nBy default, cos and derivatives will inherit an immutable setup.\nA running system will look like as follows:\n/usr/local - persistent (COS_PERSISTENT) /oem - persistent (COS_OEM) /etc - ephemeral /usr - read only / immutable This means that any changes that are not specified as cloud-init configuration are not persisting across reboots.\nYou can place persisting cloud-init files either in /oem or /usr/local/oem, cOS already supports cloud-init datasources, so you can use also load cloud-init configuration as standard userdata, depending on the platform. For more details on the cloud-init syntax, see the cloud-init configuration reference.\nNote You can check the package documentation reference for a detailed explaination of the configuration and how to override default settings. Immutability, as of other aspects, can be disabled.  ","categories":"","description":"Immutable root filesystem configuration parameters\n","excerpt":"Immutable root filesystem configuration parameters\n","ref":"/docs/reference/immutable_rootfs/","tags":"","title":"Immutable Root Filesystem"},{"body":"There are a set of core packages available in the cOS repositories which are installable with luet while creating a derivative in the Dockerfile. Those bring in new features, enhancements and core features of the cOS toolkit. Some are optional (like runtime features) while some of them are strictly necessary in order for a cOS system to boot and operate correctly.\nThis section contains reference documentation specific to core packages in the cOS repositories and the functionalities they bring.\nAll the packages available can also be browsed here.\n","categories":"","description":"This section contains documentation for core packages present in the cOS toolkit repositories\n","excerpt":"This section contains documentation for core packages present in the …","ref":"/docs/reference/packages/","tags":"","title":"Package reference documentation"},{"body":" cOS and every derivative can upgrade, rollback or just switch to different versions in runtime by using the toolkit installed inside the image.\nTo upgrade an installed system, just run elemental upgrade and reboot.\nThis will perform an upgrade based on the default derivative configuration for the image. See general configuration on how to configure defaults when building a derivative.\n“Upgrades” are not carried over the usual way of treating each single package individually: cOS considers the container image as a new system where to boot into. It will pull a new container image during this phase, which will be booted on the next reboot.\nUpgrade to a specific container image To specify a specific container image to upgrade to instead of the regular upgrade channels, run elemental upgrade --docker-image image.\nNote by default elemental upgrade --docker-image checks images against the notary registry server for valid signatures for the images tag. To disable image verification, run elemental upgrade --no-verify --docker-image.\nIntegration with System Upgrade Controller If running a kubernetes cluster on the cOS system, you can leverage the system-upgrade-controller to trigger upgrades to specific image versions, for example:\n---apiVersion:upgrade.cattle.io/v1kind:Planmetadata:name:elemental-upgradenamespace:system-upgradelabels:k3s-upgrade:serverspec:concurrency:1version:fleet-sample# Image tagnodeSelector:matchExpressions:- {key: k3s.io/hostname, operator:Exists}serviceAccountName:system-upgradecordon:true# drain:# force: trueupgrade:image:quay.io/costoolkit/test-images# Image upgrade referencecommand:- \"/usr/sbin/suc-upgrade\"See also trigger upgrades with fleet\nFrom ISO The ISO can be also used as a recovery medium: type elemental upgrade from a LiveCD. It will then try to upgrade the image of the active partition installed in the system.\nHow it works cOS during installation sets two .img images files in the COS_STATE partition:\n /cOS/active.img labeled COS_ACTIVE: Where cOS typically boots from /cOS/passive.img labeled COS_PASSIVE: Where cOS boots for fallback  Those are used by the upgrade mechanism to prepare and install a pristine cOS each time an upgrade is attempted.\n","categories":"","description":"How to run upgrades in cOS\n","excerpt":"How to run upgrades in cOS\n","ref":"/docs/getting-started/upgrading/","tags":"","title":"Upgrading"},{"body":" Section under construction.\n While building a derivative, or on a running system things can go really wrong, the guide is aimed to give tips while building derivatives and also debugging running systems.\nDon’t forget tocheck the known issues for the release you’re using.\nBefore booting, several kernel parameters can be used to help during debugging (also when booting an ISO). Those are meant to be used only while debugging, and they might defeat the concept of immutability.\nDisable Immutability By adding rd.cos.debugrw to the boot parameters read only mode will be disabled. See Immutable setup for more options.\nThe derivative will boot into RW mode, that means any change made during runtime will persist across reboots. Use this feature with caution as defeats the concept of immutability.\nrd.cos.debugrw applies only to active and passive partitions. The recovery image can’t be mutated.\nNote The changes made will persist during reboots but won’t persist across upgrades. If you need to persist changes across upgrades in runtime (for example by adding additional packages on top of the derivative image), see how to apply persistent changes.  Debug initramfs issues As derivative can ship and build their own initrd, the official debug docs contains valid information that can be used for troubleshooting.\nFor example:\n rd.break=pre-mount rd.shell: Drop a shell before setting up mount points rd.break=pre-pivot rd.shell: Drop a shell before switch-root  Recovery partition If you can boot into the system, the recovery partition can be used to reset the state of the active/passive, but can also be used to upgrade to specific images. Be sure to read the Recovery section in the docs.\nMutating derivative images It can be useful to mutate derivative images and commit a container’s file changes or settings into a new image. This allows you to debug a container by running an interactive shell, and re-use the mutated image in cOS systems. Generally, it is better to use Dockerfiles to manage your images in a documented and maintainable way. Read more about creating bootable images.\nLet’s suppose we have the derivative original image at $IMAGE and we want to mutate it. We will push it later with another name $NEW_IMAGE and use it to our node downstream.\nRun the derivative image locally, and perform any necessary change (e.g. add additional software):\n$\u003e docker run --entrypoint /bin/bash -ti --name updated-image $IMAGE Commit any changes to a new image $NEW_IMAGE:\n$\u003e docker commit updated-image $NEW_IMAGE And push the image to the container registry:\n$\u003e docker push $NEW_IMAGE In the derivative then it’s sufficient to upgrade to that image with elemental upgrade:\n$\u003e elemental upgrade --no-verify --docker-image $NEW_IMAGE Adding login keys at boot To add users key from the GRUB menu prompt, edit the boot cmdline and add the following kernel parameters:\nstages.boot[0].authorized_keys.root[0]=github:mudler\n","categories":"","description":"Stuff can go wrong. This document tries to make them right with some useful tips\n","excerpt":"Stuff can go wrong. This document tries to make them right with some …","ref":"/docs/reference/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"Examples and recipes for building cOS derivatives\n","excerpt":"Examples and recipes for building cOS derivatives\n","ref":"/docs/examples/","tags":"","title":"Examples"},{"body":"cOS derivatives have a recovery mechanism built-in which can be leveraged to restore the system to a known point. At installation time, the recovery partition is created from the installation medium.\nThe recovery system can be accessed during boot by selecting the last entry in the menu (labeled by “recovery”).\nA derivative can be recovered anytime by booting into the  recovery partition and by running elemental reset from it.\nThis command will regenerate the bootloader and the images in the COS_STATE partition by using the recovery image.\nUpgrading the recovery partition From either the active or passive system, the recovery partition can also be upgraded by running\nelemental upgrade --recovery It also supports to specify docker images directly:\nelemental upgrade --recovery --docker-image \u003cimage\u003e Upgrading the active system from the recovery The recovery system can upgrade also the active system by running elemental upgrade, and it also supports to specify docker images directly:\nelemental upgrade --recovery --docker-image \u003cimage\u003e ","categories":"","description":"How to use the recovery partition to reset the system or perform upgrades.\n","excerpt":"How to use the recovery partition to reset the system or perform …","ref":"/docs/getting-started/recovery/","tags":"","title":"Recovery"},{"body":"cOS vanilla images, like ISOs, cloud images or raw disks can be used to deploy another derivative image.\ncos-deploy cos-deploy can be used to deploy an image to the system.\nIt can be either invoked manually with cos-deploy --docker-image \u003cimg-ref\u003e or used in conjuction with a cloud-init configuration, for example consider the following cloud-init configuration file:\nname:\"Default deployment\"stages:rootfs.after:- name:\"Repart image\"layout:# It will partition a device including the given filesystem label or part label (filesystem label matches first)device:label:COS_RECOVERYadd_partitions:- fsLabel:COS_STATE# 10Gb for COS_STATE, so the disk should have at least 16Gbsize:10240pLabel:state- fsLabel:COS_PERSISTENT# unset size or 0 size means all available spacepLabel:persistentnetwork:- if:'[ -f \"/run/cos/recovery_mode\" ]'name:\"Deploy cos-system\"commands:- |# Use `cos-deploy --docker-image \u003cimg-ref\u003e` to deploy a custom image # By default latest cOS gets deployed cos-deploy --docker-image $IMAGE \u0026\u0026 shutdown -r nowThe following will first repartition the image after the rootfs stage and will run cos-deploy when booting into recovery mode. RAW vanilla disk images automatically boot by default into recovery, so the first thing upon booting is deploying the system\n","categories":"","description":"How to deploy derivatives images from cOS vanilla images\n","excerpt":"How to deploy derivatives images from cOS vanilla images\n","ref":"/docs/getting-started/deploy/","tags":"","title":"Deploying"},{"body":"cOS allows to (optionally) add features that can be disabled/enabled in runtime, provided by  system/cos-features  .\nCloud-init files stored in /system/features are read by cos-feature and allow to interactively enable or disable them in a running system, for example:\n$\u003e cos-feature list ==================== cOS features list To enable, run: cos-feature enable \u003cfeature\u003e To disable, run: cos-feature disable \u003cfeature\u003e ==================== - vagrant (enabled) - ... ...  Note /system/features is the default path which can be customized in the cOS configuration file by specifying it with COS_FEATURESDIR.  By default cOS ships the vagrant featureset - when enabled will automatically create the default vagrant user which is generally used to create new Vagrant boxes.\nIf you don’t need cos-features you can avoid installing  system/cos-features  , it’s optional.\nAdding or removing features To either add or remove the available features, delete the relevant files in the /system/features folder of the derivative prior to build the container image.\n","categories":"","description":"Add features that can be enabled and disabled on runtime\n","excerpt":"Add features that can be enabled and disabled on runtime\n","ref":"/docs/customizing/runtime_features/","tags":"","title":"Runtime features"},{"body":"Installing required dependencies for local build To get requirements installed locally, run:\n$\u003e make deps or you need:\n luet luet-makeiso squashfs-tools  zypper in squashfs on SLES or openSUSE   xorriso  zypper in xorriso on SLES or openSUSE   yq (version 4.x), installed via packages/toolchain/yq (optional) jq, installed via packages/utils/jq (optional)  Note: Running make deps will install only luet, luet-makeiso, yq and jq. squashfs-tools and xorriso needs to be provided by the OS.\nManually install dependencies To install luet locally, you can also run as root:\n# curl https://raw.githubusercontent.com/rancher-sandbox/cOS-toolkit/master/scripts/get_luet.sh | sh or build luet from source).\nYou can find more luet components in the official Luet repository.\nluet-makeiso luet-makeiso comes with cOS-toolkit and can be installed with luet locally:\n$\u003e luet install -y toolchain/luet-makeiso You can also grab the binary from luet-makeiso releases.\nyq and jq yq (version 4.x) and jq are used to retrieve the list of packages to build in order to produce the final ISOs. Those are not strictly required, see the Note below.\nThey are installable with:\n$\u003e luet install -y utils/jq toolchain/yq Note: yq and jq are just used to generate the list of packages to build, and you don’t need to have them installed if you manually specify the packages to be compiled.\n","categories":"","description":"Building prerequisites\n","excerpt":"Building prerequisites\n","ref":"/docs/development/dependencies/","tags":"","title":"Build requirements"},{"body":"cOS toolkit High level Architecture This page tries to encompass the cos-toolkit structure and the high level architecture, along with all the involved components.\nDesign goals  Blueprints to build immutable Linux derivatives from container images A workflow to maintain, support and deliver custom-OS and upgrades to end systems Derivatives have the same “foundation” manifest - easy to customize on top, add packages: systemd, dracut and grub as a foundation stack. Upgrades delivered with container registry images ( also workflow with docker run \u0026\u0026 docker commit supported! ) The content of the container image is the system which is booted.  High level overview cOS-Toolkit encompasses several components required for building and distributing OS images. This issue summarize the current state, and how we plan to integrate them in a single CLI to improve the user experience.\ncOS-Toolkit is also a manifest, which includes package definitions of how the underlying OS is composed. It forms an abstraction layer, which is then translated to Dockerfiles and built by our CI (optionally) for re-usal. A derivative can be built by parts of the manifest, or reusing it entirely, container images included.\nThe fundamental phases can be summarized in the following steps:\n Build packages from container images (and optionally keep build caches) Extract artefacts from containers Add metadata(s) and create a repository (optionally) publish the repository and the artefacts  The developer of the derivative applies a customization layer during build, which is an augmentation layer in the same form of cos-toolkit itself.\nDistribution The OS delivery mechanism is done via container registries. The developer that wants to provide upgrades for the custom OS will push the resulting container images to the container registry. It will then be used by the installed system to pull upgrades from.\nUpgrade mechanism There are two different upgrade mechanisms available that can be used from a maintainer perspective: (a) release channels or (b) providing a container image reference ( e.g. my.registry.com/image:tag ) that can be tweaked in the customization phases to achieve the desired effect.\n","categories":"","description":"High level architecture of cOS and its components.\n","excerpt":"High level architecture of cOS and its components.\n","ref":"/docs/reference/high_level_architecture/","tags":"","title":"High level architecture"},{"body":"","categories":"","description":"cOS tutorials and real life use-case samples\n","excerpt":"cOS tutorials and real life use-case samples\n","ref":"/docs/tutorials/","tags":"","title":"Tutorials"},{"body":" Github project for a short-term Roadmap  Samples repositories  Build a derivative and upgrade it with fleet  ","categories":"","description":"References for cOS derivatives, like common featuresets, high level architecture\n","excerpt":"References for cOS derivatives, like common featuresets, high level …","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":" For Vagrant Boxes, OVA and QEMU images at the moment we are relying on packer templates.\nThe cOS vanilla images can be used as input to Packer to deploy pristine images of the user container image with cos-deploy.\n","categories":"","description":"Building VBox, OVA, QEMU, etc. images of your derivative with Packer\n","excerpt":"Building VBox, OVA, QEMU, etc. images of your derivative with Packer\n","ref":"/docs/creating-derivatives/packer/","tags":"","title":"Building images with Packer"},{"body":"Welcome!\nThe cOS (containerized OS) distribution is entirely built over GitHub. You can check the pipelines in the .github folder to see how the process looks like.\nRepository layout  packages: contain packages definition for luet values: interpolation files, needed only for multi-arch and flavor-specific build assets: static files needed by the iso generation process packer: Packer templates tests: cOS test suites manifest.yaml: Is the manifest needed used to generate the ISO and additional packages to build  Forking and test on your own By forking the cOS-toolkit repository, you already have the Github Action workflow configured to start building and pushing your own cOS fork.\nThe only changes required to keep in mind for pushing images:\n set DOCKER_PASSWORD and DOCKER_USERNAME as Github secrets, which are needed to push the resulting container images from the pipeline. Tweak or set the Makefile’s REPO_CACHE and FINAL_REPO accordingly. Those are used respectively for an image used for cache, and for the final image reference.  Those are not required for building - you can disable image push (--push) from the Makefile or just by specifying e.g. BUILD_ARGS=--pull when calling the make targets.\nBuilding locally cOS has a container image which can be used to build cOS locally in order to generate the cOS packages and the cOS iso from your checkout.\nFrom your git folder:\n$\u003e docker build -t cos-builder . $\u003e docker run --privileged=true --rm -v /var/run/docker.sock:/var/run/docker.sock -v $PWD:/cOS cos-builder or use the .envrc file:\n$\u003e source .envrc $\u003e cos-build Build all packages locally Building locally has a set of dependencies that should be satisfied.\nThen you can run\n# make build as root\nTo clean from previous runs, run make clean.\nNote: The makefile uses yq and jq to retrieve the packages to build from the iso specfile.\nIf you don’t have jq and yq installed, you must pass by the packages manually with PACKAGES (e.g. PACKAGES=\"system/cos live/systemd-boot live/boot live/syslinux\").\nYou might want to build packages running as root or sudo -E if you intend to preserve file permissions in the resulting packages (mainly for xattrs, and so on).\nBuild ISO If using SLES or openSUSE, first install the required deps:\n# zypper in -y squashfs xorriso dosfstools and then, simply run\n# make local-iso Testing ISO changes To test changes against a specific set of packages, you can for example:\n# make PACKAGES=\"toolchain/yq\" build local-iso root is required because we want to keep permissions on the output packages (not really required for experimenting).\nRun with qemu After you have the iso locally, run\n $\u003e QEMU=qemu-system-x86_64 make run-qemu This will create a disk image at .qemu/drive.img and boot from the ISO.\n If the image already exists, it will NOT be overwritten.\nYou need to run an explicit make clean_run to wipe the image and start over.\n Installing With a fresh drive.img, make run-qemu will boot from ISO. You can then log in as root with password cos and install cOS on the disk image with:\n# elemental install /dev/sda Running After a successful installation of cOS on drive.img, you can boot the resulting sytem with\n $\u003e QEMU_ARGS=\"-boot c\" make run-qemu Run tests Requires: Virtualbox or libvirt, vagrant, packer\nWe have a test suite which runs over SSH.\nTo create the vagrant image:\n $\u003e PACKER_ARGS=\"-var='feature=vagrant' -only virtualbox-iso.cos\" make packer To run the tests:\n $\u003e make test ","categories":"","description":"How to build cOS?\n","excerpt":"How to build cOS?\n","ref":"/docs/development/","tags":"","title":"Development"},{"body":"What is cOS? cOS is a toolkit which allows container images to be bootable in VMs, baremetals, embedded devices, and much more.\ncOS allows to create meta-Linux derivatives which are configured throughout cloud-init configuration files and are immutable by default.\ncOS and derivatives shares a common feature set, can be upgraded in a OTA-alike* style, and upgrades are delivered with standard container registries.\ncOS comes also with vanilla images that can be used to boot directly container images built with the toolkit.\nNote Note here we refer to “OTA-alike” merely as a method of distributing upgrades. It does not involve any specific setup (either wireless or cabled works) except an network connection to the registry where images are stored. Updates are delivered from one central location (the container registry) which hosts all images available for the clients to pull from.  Why cOS? cOS allows to create custom OS versions in your cluster with standard container images with a high degree of customization. It can also be used in its vanilla form - cOS enables then everyone to build their own derivative and access it in various formats.\nTo build a bootable image is as simple as running docker build.\n What is it good for?: Embedded, Cloud, Containers, VM, Baremetals, Servers, IoT, Edge  Design goals  A Manifest for container-based OS. It contains just the common bits to make a container image bootable and to be upgraded from, with few customization on top Everything is an OCI artifact from the ground-up Immutable-first, but with a flexible layout Cloud-init driven Based on systemd Built and upgraded from containers - It is a single image OS! OTA updates Easy to customize Cryptographically verified instant switch from different versions recovery mechanism with cOS vanilla images (or bring your own)  Mission The cOS-toolkit project is under the Elemental umbrella.\nElemental provides a unique container based approach to define the system lifecycle of an immutable Linux derivative, without any string attached to a specific Linux distribution.\nAt its heart, Elemental is the abstraction layer between Linux distro management and the specific purpose of the OS.\nElemental empowers anyone to create derivatives from standard OCI images. Frees whoever wants to create a Linux derivative from handling the heavy bits of packaging and managing entire repositories to propagate upgrades, simplifying the entire process by using container images as base for OS. At the same time, Elemental provides an highly integrated ecosystem which is designed to be container-first, cloud native, and immutable. Anyone can tweak Elemental derivatives from the bottom-up to enable and disable its featureset.\nAs the Elemental team, the os2 project is our point of reference.\nos2 is a complete derivative built with Elemental tied with the rancher ecosystem and full cycle node management solution with Kubernetes.\nWe are supporting directly and indirectly os2 within changes also in the Elemental ecosystem.\nos2 is our main show-case, and as the Elemental team we are committed to it. It encompasses several technologies to create a Kubernetes-focused Linux derivative which lifecycle is managed entirely from Kubernetes itself, Secure Device Onboarding included, and automatic provisioning via cloud-init.\n","categories":"","description":"","excerpt":"What is cOS? cOS is a toolkit which allows container images to be …","ref":"/docs/","tags":"","title":"Documentation"},{"body":" Note  meta/cos-core  is part of the cOS toolkit repository and can be installed with:\nluet install -y meta/cos-core in the derivative’s Dockerfile.\n cos-core is a meta collection which pulls the toolchain (elemental-cli and luet) and the cos modules (installer,cos-setup, immutable-rootfs, base-dracut-modules and grub2-config).\nA meta/cos-core-fips variant is available to pull the toolchain compiled with fips enabled.\n","categories":"","description":"Documentation for the meta/cos-core package\n","excerpt":"Documentation for the meta/cos-core package\n","ref":"/docs/reference/packages/meta-packages/meta-cos-core/","tags":"","title":"meta/cos-core"},{"body":" Note  meta/cos-minimal  is part of the cOS toolkit repository and can be installed with:\nluet install -y meta/cos-minimal in the derivative’s Dockerfile.\n This meta collection includes the cos-core collection and additionally a set of default cloud-config files installed in /system/oem. The cloud-config files are setting defaults for upgrades, system user and password, default network setup and the immutability layer.\nA meta/cos-minimal-fips variant is available to pull the toolchain compiled with fips enabled.\n","categories":"","description":"Documentation for the meta/cos-minimal package\n","excerpt":"Documentation for the meta/cos-minimal package\n","ref":"/docs/reference/packages/meta-packages/meta-cos-minimal/","tags":"","title":"meta/cos-minimal"},{"body":" Note  meta/cos-modules  is part of the cOS toolkit repository and can be installed with:\nluet install -y meta/cos-modules in the derivative’s Dockerfile.\n The cos modules pulls the following packages:\n installer (which ships cos-installer, cos-upgrade, cos-reset etc. ) cos-setup (which ships the system config services for systemctl) immutable-rootfs (dracut modules) base-dracut-modules and grub2-config (dracut and grub2 configuration)  ","categories":"","description":"Documentation for the meta/cos-modules package\n","excerpt":"Documentation for the meta/cos-modules package\n","ref":"/docs/reference/packages/meta-packages/meta-cos-modules/","tags":"","title":"meta/cos-modules"},{"body":" Note  meta/cos-verify  is part of the cOS toolkit repository and can be installed with:\nluet install -y meta/cos-verify in the derivative’s Dockerfile.\n The cos-verify meta package pulls cosign and the luet-cosign plugin to enable container image signature verification.\n","categories":"","description":"Documentation for the meta/cos-verify package\n","excerpt":"Documentation for the meta/cos-verify package\n","ref":"/docs/reference/packages/meta-packages/meta-cos-verify/","tags":"","title":"meta/cos-verify"},{"body":" Note  system/dracut-initrd  is part of the cOS toolkit repository and can be installed with:\nluet install -y system/dracut-initrd in the derivative’s Dockerfile.\n This package provides an initrd generated by dracut which should be suitable for generic machines.\nIn cases where Derivative wish to re-use the same initrd and kernel from cOS vanilla images, this package can be used in conjuction with system/kernel.\n","categories":"","description":"Documentation for the system/dracut-initrd package\n","excerpt":"Documentation for the system/dracut-initrd package\n","ref":"/docs/reference/packages/system-dracut-initrd/","tags":"","title":"system/dracut-initrd"},{"body":" Note  system/immutable-rootfs  is part of the cOS toolkit repository and can be installed with:\nluet install -y system/immutable-rootfs in the derivative’s Dockerfile.\n This package ships the immutable-rootfs dracut module responsible of mounting the root tree during boot time with the immutable specific setup. The immutability concept refers to read only root (/) system. To ensure the linux OS is still functional certain paths or areas are required to be writable, in those cases an ephemeral overaly tmpfs is set in place. Additionaly, the immutable rootfs module can also mount a custom list of device blocks with read write permissions, those are mostly devoted to store persistent data.\nThe dracut module is mostly configured via kernel command line parameters or via the /run/cos/cos-layout.env environment file.\nThese are the read write paths the module mounts as part of the overlay ephemeral tmpfs: /etc, /root, /home, /opt, /srv, /usr/local and /var.\nThese paths will be all ephemeral unless there is a block device configured to be mounted in the same path.\nIt is important to remark all the immutable root configuration is applied in initrd before switching root and after rootfs cloud-init stage but before initramfs stage. So immutable rootfs configuration via cloud-init using the /run/cos/cos-layout.env file is only effective if called in any of the rootfs.before, rootfs or rootfs.after cloud-init stages.\nKernel configuraton paramters The immutable rootfs can be configured witht he following kernel parameters:\n  cos-img/filename=\u003cimgfile\u003e: This is one of the main parameters, it defines the location of the image file to boot from.\n  rd.cos.overlay=tmpfs:\u003csize\u003e: This defines the size of the tmpfs used for the ephemeral overlayfs. It can be expressed in MiB or as a % of the available memory. Defaults to rd.cos.overlay=tmpfs:20% if not present.\n  rd.cos.overlay=LABEL=\u003cvol_label\u003e: Optionally and mostly for debugging purposes the overlayfs can be mounted on top of a persistent block device. Block devices can be expressed by LABEL (LABEL=\u003cblk_label\u003e) or by UUID (UUID=\u003cblk_uuid\u003e)\n  rd.cos.mount=LABEL:\u003cblk_label\u003e:\u003cmountpoint\u003e: This option defines a persistent block device and its mountpoint. Block devices can also be defined by UUID (UUID=\u003cblk_uuid\u003e:\u003cmountpoint\u003e). This option can be passed multiple times.\n  rd.cos.oemtimeout=\u003cseconds\u003e: cOS by default assumes the existence of a persistent block device labelled COS_OEM which is used to keep some configuration data (mostly cloud-init files). The immutable rootfs tries to mount this device at very early stages of the boot even before applying the immutable rootfs configs. It done this way to enable to configure the immutable rootfs module within the cloud-init files. As the COS_OEM device might not be always present the boot process just continues without failing after a certain timeout. This option configures such a timeout. Defaults to 10s.\n  rd.cos.debugrw: This is a boolean option, true if present, false if not. This option sets the root image to be mounted as a writable device. Note this completely breaks the concept of an immutable root. This is helpful for debugging or testing purposes, so changes persist across reboots.\n  rd.cos.disable: This is a boolean option, true if present, false if not. It disables the execution of any immutable rootfs module logic at boot.\n  Configuration with an environment file The immutable rootfs can be configured with the /run/cos/cos-layout.env environment file. It is important to note that all the immutable root configuration is applied in initrd before switching root and after rootfs cloud-init stage but before initramfs stage. So immutable rootfs configuration via cloud-init using the /run/cos/cos-layout.env file is only effective if called in any of the rootfs.before, rootfs or rootfs.after cloud-init stages.\nIn the environment file few options are available:\n  VOLUMES=LABEL=\u003cblk_label\u003e:\u003cmountpoint\u003e: This variable expects a block device and it mountpoint pair space separated list. The default cOS configuration is:\nVOLUMES=\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"\n  OVERLAY: It defines the underlaying device for the overlayfs as in rd.cos.overlay= kernel parameter.\n  DEBUGRW=true: Sets the root (/) to be mounted with read/write permissions.\n  MERGE=true: Sets makes the VOLUMES values to be merged with any other volume that might have been defined in the kernel command line. The merging criteria is simple: any overlapping volume is overwritten all others are appended to whatever was already defined as a kernel parameter. If not defined defaults to true.\n  RW_PATHS: This is a space separated list of paths. These are the paths that will be used for the ephemeral overlayfs. These are the paths that will be mounted as overlay on top of the OVERLAY (or rd.cos.overlay) device. Default value is:\nRW_PATHS=\"/etc /root /home /opt /srv /usr/local /var\" Note: as those paths are overlayed with an ephemeral mount (tmpfs), additional data wrote on those location won’t be available on subsequent boots.\n  PERSISTENT_STATE_TARGET: This is the folder where the persistent state data will be stored, if any. Default value is /usr/local/.state.\n  PERSISTENT_STATE_PATHS: This is a space separated list of paths. These are the paths that will become writable and store its data inside PERSISTENT_STATE_TARGET. By default this variable is empty, which means no persistent state area is created or used.\nNote: The specified paths needs either to exist or be located in an area which is writeable ( for example, inside locations specified with RW_PATHS). The dracut module will attempt to create non-existant directories, but might fail if the mountpoint where are located is read-only.\n  PERSISTENT_STATE_BIND=\"true|false\": When this variable is set to true the persistent state paths are bind mounted (instead of using overlayfs) after being mirrored with the original content. By default this variable is set to false.\n  Note that persistent state are is setup once the ephemeral paths and persistent volumes are mounted. Persistent state paths can’t be an already existing mount point. If the persistent state requires any of the paths that are part of the ephemeral area by default, then RW_PATHS needs to be defined to avoid overlapping paths.\nFor exmaple a common cOS configuration can be expressed as part of the cloud-init configuration as follows:\nname:examplestage:rootfs:- name:\"Layout configuration\"environment_file:/run/cos/cos-layout.envenvironment:VOLUMES:\"LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/usr/local\"OVERLAY:\"tmpfs:25%\"","categories":"","description":"Documentation for the system/immutable-rootfs package\n","excerpt":"Documentation for the system/immutable-rootfs package\n","ref":"/docs/reference/packages/system-immutable-rootfs/","tags":"","title":"system/immutable-rootfs"},{"body":" Note  utils/installer  is part of the cOS toolkit repository and can be installed with:\nluet install -y utils/installer in the derivative’s Dockerfile.\n This package provides the following utilities:\n cos-installer ( wrapper of cos --install ) cos-deploy ( wrapper of elemental install ) cos-reset ( wrapper of elemental reset ) cos-rebrand ( wrapper of cos --rebrand ) cos-upgrade ( wrapper of elemental upgrade ) suc-upgrade ( wrapper of cos --upgrade )  ","categories":"","description":"Documentation for the utils/installer package\n","excerpt":"Documentation for the utils/installer package\n","ref":"/docs/reference/packages/utils-installer/","tags":"","title":"utils/installer"},{"body":" Note  system/kernel  is part of the cOS toolkit repository and can be installed with:\nluet install -y system/kernel in the derivative’s Dockerfile.\n This package provides the default distribution kernel which should be suitable for generic machines.\nIn cases where Derivative wish to re-use the same initrd and kernel from cOS vanilla images, this package can be used in conjuction with system/dracut-initrd.\n","categories":"","description":"Documentation for the system/kernel package\n","excerpt":"Documentation for the system/kernel package\n","ref":"/docs/reference/packages/system-kernel/","tags":"","title":"system/kernel"},{"body":" Note  meta/toolchain  is part of the cOS toolkit repository and can be installed with:\nluet install -y meta/toolchain in the derivative’s Dockerfile.\n A meta package for luet and elemental-cli.\n","categories":"","description":"Documentation for the meta/toolchain package\n","excerpt":"Documentation for the meta/toolchain package\n","ref":"/docs/reference/packages/meta-packages/meta-toolchain/","tags":"","title":"meta/toolchain"},{"body":"  Immutable Linux Derivatives at your fingertips  cOS is a toolkit to build, ship and maintain cloud-init driven Linux derivatives with containers..  Learn More    --   Great For Embedded Appliances Edge True DevOps    Why Use cOS toolkit ?  Built for DevOps cOS toolkit allows to maintain custom Linux derivatives in a GitOps style with container registries.\n  Operational Happiness Upgrade machines from container images in OTA style - manually or automatically within kubernetes.\n  Bring your own OS A common featureset shared between cOS derivatives, completely pluggable.\n  Single Image OS cOS derivatives are single container image OS that are bootable from a system. They follow an OTA update style and gets upgrades from regular container registries.\n       How it Works\n         Get Started Step 1.git clone https://github.com/rancher-sandbox/cos-toolkit Step 2.docker build -t example examples/standard Step 3.elemental upgrade example  That’s it! With just a few commands, you have your own custom OS.\n Learn More Ready to upgrade?\nBuild with cOS toolkit today     ","categories":"","description":"Immutable Linux Derivatives at your fingertips","excerpt":"Immutable Linux Derivatives at your fingertips","ref":"/","tags":"","title":"cOS toolkit"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]